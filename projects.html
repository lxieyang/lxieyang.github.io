---
layout: page
title: Projects
navigation_weight: 200
image_path_mobile_paper_reader: '/resources/images/works/paperreader/mobile_paper_reader_preview.png'
image_path_crowd_ml: '/resources/images/works/crowd_ml/interface_micro.png'
image_path_easydraw: '/resources/images/works/easydraw/interface.png'
image_path_denso: '/resources/images/works/denso/vatic.png'
image_path_hico: '/resources/images/works/img-rec/preview_amazon_project_1.png'
image_path_travelife: '/resources/images/works/travelife/preview_travel_life_2.png'
image_path_laserguitar: '/resources/images/works/guitar/preview_guitar_1.png'
---

<br>


<div class="work-container">


<div class="school-title" id="school-cmu">
	Carnegie Mellon University
</div>

<div class="project" id="porj-mobile_paper_reader">
		<a name="mobile_paper_reader" id="mobile_paper_reader"></a>
		<div class="project-title">
			<a href="{{ site.project_links.mobile_paper_reader }}">{{ site.project_titles.mobile_paper_reader }}</a>
			<div class="project-year">2017  &middot;  self-motivated</div>
		</div>
		<div class="row">
			<div class="col-md-5">
				<a href="{{ site.project_links.mobile_paper_reader }}">
					<img class="proj-image-preview" src="{{ page.image_path_mobile_paper_reader }}" alt="crowd_ml">
				</a>
			</div>
			<div class="col-md-7">
					Ever experienced reading research papers on your phone? It sucked, right? The phone screens are just too small for those magnificant pdf documents people worked so hard to format.<br><br>
					With this app, you could just put in the link to the paper you want to read, wait a little while for the app to parse the pdf document using some advanced text extraction algorithms from <a href="https://indico.io">indico</a>, and then read the paper like you are reading a normal article on your phone screen. To illustrate, you just went from the left to the right.<br><br>
					<em>This project is on-going!</em>
			</div>
		</div>
	</div>











<br><hr>
<div class="school-title" id="school-um">
	University of Michigan, Ann Arbor
</div>

<div class="project" id="porj-crowd_ml">
	<a name="crowd_ml" id="crowd_ml"></a>
	<div class="project-title">
    <a href="{{ site.project_links.crowd_ml }}">{{ site.project_titles.crowd_ml }}</a>
    <div class="project-year">2016  &middot;  research</div>
  </div>
	<div class="row">
		<div class="col-md-5">
			<a href="{{ site.project_links.crowd_ml }}">
	      <img class="proj-image-preview" src="{{ page.image_path_crowd_ml }}" alt="crowd_ml">
	    </a>
		</div>
		<div class="col-md-7">
			In this project, we seek to investigate the impact of added subclass information on the performances of traditional machine learning classifiers. My expertise in interactive system building and crowdsourcing comes in when we need to develop an efficient labeling tool for gathering superclass/subclass information of the images in the dataset from the crowd, and in the meantime evaluate their performances and fine-tune our system accordingly. In the future, we vision to deliver a web-based interactive system on image labeling that features front-end multi-worker collaborating in real time and back-end machine learning classifying and re-sampling on the fly.
		</div>
	</div>
</div>

<div class="project" id="porj-easydraw">
	<a name="easydraw" id="easydraw"></a>
	<div class="project-title">
    <a href="{{ site.project_links.easydraw }}">{{ site.project_titles.easydraw }}</a>
    <div class="project-year">2016  &middot;  course project</div>
  </div>
	<div class="row">
		<div class="col-sm-5">
			<a href="{{ site.project_links.easydraw }}">
	      <img class="proj-image-preview" src="{{ page.image_path_easydraw }}" alt="easydraw">
	    </a>
		</div>
		<div class="col-sm-7">
			In this project, we seek to develop a figure and diagram editing application on iPad that fulfill the needs of the physically incapacitated. Considering the significant loss of fine motor skills of our customers, all the functionality such as object creation, navigation, and scaling can all be accessed using a single gesture: tapping. I lead the design and development of the application. We aim to make this application a member of the assistive technology family.
		</div>
	</div>
</div>

<div class="project" id="porj-denso">
	<a name="denso" id="denso"></a>
	<div class="project-title">
    <a href="{{ site.project_links.denso }}"> {{ site.project_titles.denso }} </a>
    <div class="project-year">2016 &middot; research</div>
  </div>
	<div class="row">
		<div class="col-sm-5">
			<a href="{{ site.project_links.denso }}">
	      <img class="proj-image-preview" src="{{ page.image_path_denso }}" alt="denso">
	    </a>
		</div>
		<div class="col-sm-7">
			This project aims to extract physical driving conditions and parameters for 3D-reconstruction from real, unconstrained vehicle crash videos available on public video sharing websites with the help of a human in the loop. I developed a reconfigurable, web-based vehicle crash scene annotation UI that enables crowd workers to efficiently and effectively provide measurement values of objects in a crash scene, as well as a reusable annotation server backend that recruits crowd workers for real-time tasks, collects responses and visualizes the obtained data samples.
		</div>
	</div>
</div>

<div class="project" id="porj-hico">
	<a name="hico" id="hico"></a>
	<div class="project-title">
    <a href="{{ site.project_links.hico }}"> {{ site.project_titles.hico }} </a>
    <div class="project-year">2015 - 2016 &middot; research</div>
  </div>
	<div class="row">
		<div class="col-sm-5">
			<a href="{{ site.project_links.hico }}">
	      <img class="proj-image-preview" src="{{ page.image_path_hico }}" alt="hico">
	    </a>
		</div>
		<div class="col-sm-7">
			In this project, we study the problem of detecting human-object interactions (HOI) in static images, defined as predicting a human and an object bounding box with an interaction class label that connects them. HOI detection is a fundamental problem in computer vision as it provides semantic information about the interactions among the detected objects. We introduce HICO-DET, a new large benchmark for HOI detection, by augmenting the current HICO classification benchmark with instance annotations. We propose Human-Object Region-based Convolutional Neural Networks (HO-RCNN), a novel DNN-based framework for HOI detection. At the core of our HO-RCNN is the Interaction Pattern, a novel DNN input that characterizes the spatial relations between two bounding boxes. We validate the effectiveness of our HO-RCNN using HICO-DET. Experiments demonstrate that our HO-RCNN, by exploiting human-object spatial relations through Interaction Patterns, significantly improves the performance of HOI detection over baseline approaches.
		</div>
	</div>
	<br>
	<div class="project-pub">
		<div class="row">
			<div class="col-md-12">
				<h4>Publication</h4>
			</div>
		</div>
		<hr>
		<div class="row">
			<div class="col-md-12">
				<div class="paper-title pub-element">
					Learning to Detect Human-Object Interactions
				</div>
				<div class="authors pub-element">
					Yu-Wei Chao, Yunfan Liu, <span class="author-important">Xieyang Liu</span>, Huayi Zeng, Jia Deng.
				</div>
				<div class="publication-location pub-element">
					<span class="conference">IEEE Winter Conference on Applications of Computer Vision (WACV)</span>, <span class="conference-year">2018</span>.
				</div>
				<div class="data pub-element">
					[ <a data-toggle="collapse" href="#hico-abstract" aria-expanded="false" aria-controls="hico-abstract">Abstract</a> ]          
					[ <a data-toggle="collapse" href="#hico-bibtex" aria-expanded="false" aria-controls="hico-bibtex">BibTex</a> ]
					[ <a href="https://arxiv.org/abs/1702.05448" target="_blank">arXiv</a> ]
					[ <a href="http://www-personal.umich.edu/~ywchao/hico/" target="">Project</a> ]
					[ <a href="http://napoli18.eecs.umich.edu/public_html/data/hico_20160224_det.tar.gz" target="_blank">Data</a> ]
					[ <a href="https://github.com/ywchao/ho-rcnn" target="_blank">Code</a> ]
					[ <a href="https://github.com/ywchao/hoi-det-ui" target="_blank">UI code</a> ]
				</div>
				<div id="hico-abstract" class="collapse paper-abstract publication-collapse">
					In this paper we study the problem of detecting human-object interactions (HOI) in static images, defined as predicting a human and an object bounding box with an interaction class label that connects them. HOI detection is a fundamental problem in computer vision as it provides semantic information about the interactions among the detected objects. We introduce HICO-DET, a new large benchmark for HOI detection, by augmenting the current HICO classification benchmark with instance annotations. We propose Human-Object Region-based Convolutional Neural Networks (HO-RCNN), a novel DNN-based framework for HOI detection. At the core of our HO-RCNN is the Interaction Pattern, a novel DNN input that characterizes the spatial relations between two bounding boxes. We validate the effectiveness of our HO-RCNN using HICO-DET. Experiments demonstrate that our HO-RCNN, by exploiting human-object spatial relations through Interaction Patterns, significantly improves the performance of HOI detection over baseline approaches. 
				</div>
				<div id="hico-bibtex" class="collapse publication-collapse">
					<pre class="paper-bibtex">
@INPROCEEDINGS{chao:wacv2018,
	author = {Yu-Wei Chao and Yunfan Liu and Xieyang Liu and Huayi Zeng and Jia Deng},
	title = {Learning to Detect Human-Object Interactions},
	booktitle = {Proceedings of the IEEE Winter Conference on Applications of Computer Vision},
	year = {2018},
}
					</pre>
				</div>
			</div>
		</div>
	</div>
</div>













<br><hr>
<div class="school-title" id="school-sjtu">
	University of Michigan - Shanghai Jiao Tong University Joint Institute
</div>

<div class="project" id="porj-travelife">
	<a name="travelife" id="travelife"></a>
	<div class="project-title">
    <a href="{{ site.project_links.travelife }}">{{ site.project_titles.travelife }}</a>
    <div class="project-year">2015 &middot; course project</div>
  </div>
	<div class="row">
		<div class="col-sm-5">
			<a href="{{ site.project_links.travelife }}">
	      <img class="proj-image-preview" src="{{ page.image_path_travelife }}" alt="travelife">
	    </a>
		</div>
		<div class="col-sm-7">
			In this project, we created a web-app and its corresponding customer services that facilitates travelling in China. Tourists could use our web-app to input their travel preferences, and instantly get matched to the most suitable professional local guide. There are also evaluation systems in place to guarantee the quality of our customer services and people's travel experience.
		</div>
	</div>
</div>

<div class="project" id="porj-laserguitar">
	<a name="laserguitar" id="laserguitar"></a>
	<div class="project-title">
    <a href="{{ site.project_links.laserguitar }}">{{ site.project_titles.laserguitar }}</a>
    <div class="project-year">2014 &middot; self-motivated</div>
  </div>
	<div class="row">
		<div class="col-sm-5">
			<a href="{{ site.project_links.laserguitar }}">
	      <img class="proj-image-preview" src="{{ page.image_path_laserguitar }}" alt="laserguitar">
	    </a>
		</div>
		<div class="col-sm-7">
			Ever imagined playing a guitar with "laser" strings? You've just got yourself a chance!
			In this project, I replaced ordinary guitar strings with laser beams, and used an electronic stereo system as the resonator. I designed and implemented the entire electronic control system so that it could well simulate an acoustic guitar.
		</div>
	</div>
</div>




</div>


<!--
<div class="posts">
  {% for post in paginator.posts %}
  <div class="post">
    <h1 class="post-title">
      <a href="{{ post.url }}">
        {{ post.title }}
      </a>
    </h1>

    <span class="post-date">{{ post.date | date_to_string }}</span>

    {{ post.content }}
  </div>
  {% endfor %}
</div>

<div class="pagination">
  {% if paginator.next_page %}
    <a class="pagination-item older" href="{{ site.baseurl }}page{{paginator.next_page}}">Older</a>
  {% else %}
    <span class="pagination-item older">Older</span>
  {% endif %}
  {% if paginator.previous_page %}
    {% if paginator.page == 2 %}
      <a class="pagination-item newer" href="{{ site.baseurl }}">Newer</a>
    {% else %}
      <a class="pagination-item newer" href="{{ site.baseurl }}page{{paginator.previous_page}}">Newer</a>
    {% endif %}
  {% else %}
    <span class="pagination-item newer">Newer</span>
  {% endif %}
</div>
-->

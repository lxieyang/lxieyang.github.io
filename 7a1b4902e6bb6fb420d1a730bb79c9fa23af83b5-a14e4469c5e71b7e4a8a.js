(self.webpackChunkmichael_personal_website=self.webpackChunkmichael_personal_website||[]).push([[562],{2326:function(e,n,t){"use strict";t.d(n,{S:function(){return L},E:function(){return A}});var a=t.p+"static/hico-preview-a1690198c87edb588bcf8e25cf6cf219.png",o=t.p+"static/unakite-vlhcc-workshop-6b4cbaf264ce7972331b1e40dd54d0b3.png",r=t.p+"static/stackoverflow-vlhcc-workshop-preview-e43b56ff074bdcdc1edbc215973ba025.jpg",i=t.p+"static/av_anno-d4f3ae94abd63c249624fc4ca9bff54b.png",s=t.p+"static/unakite-uist-2019-2979a6ee4c1d01a61fed4c068b189d6e.png",l=t.p+"static/covidcast-pnas-2021-d797c3f4463a3507c2991ffe2954d6fa.png",c=t.p+"static/strata-cscw-2021-8abeb8f798bd549c0dff11b7e4d54ed0.jpg",u=t.p+"static/tabsdo-uist-2021-f954af2f2336f5ac7e66a3f3cc213626.jpg",d=t.p+"static/crystalline-chi-2022-d6e9d10fc342b424705b0d7db3a1d3d3.png",p=t.p+"static/adamite-chi-2022-1b447d3dcec1642a0aeeb92ebc2d988d.png",h=t.p+"static/multimodal-assets-2022-4c4440c1d214091e88ceb12d4c83236a.png",f=t.p+"static/wigglite-uist-2022-2052f345b11a0af3a041884376350e61.png",g=t.p+"static/llmgam-chi-2023-102febb075563c4f90724971c66c76e9.png",m=t.p+"static/pinmi-chi-2023-c3aa0290533435e5b6d89430fc6ae05a.png",b=t.p+"static/llm-comparator-2024-93814af1f7327638f045b88993d83861.png",v=t.p+"static/llm-constraints-2024-c82da2b6ed617bfaff3765a3e37b440d.jpg",y=t.p+"static/cooking-chi-2024-60eb74288ab30ea466738d27524f3721.jpg",w=t.p+"static/selenite-chi-2024-cc7c31e3f8ff8792e2de72499ae53118.jpg",k=t.p+"static/mobilemaker-vlhcc-2024-7348ed2894119b4f7fee88ad6849b85d.jpg",x=t.p+"static/llmcomparator-vis-2024-864c23e8ab9f2722f16441e29b479306.jpg",C=t.p+"static/gensors-iui-2025-aaed499b31ed8e23bc635e0317395cbe.jpg",I=t.p+"static/industryllm-chi-2025-902073e43cf3c58eaeb059b6fbfaf485.jpg";const L=t(759).Im+"/pubs",A={publications:[{title:"Learning to Detect Human-Object Interactions",type:"conference",conference:"WACV",conferenceFullName:"IEEE Winter Conference on Applications of Computer Vision (WACV)",conferenceTag:"WACV 2018",year:2018,month:3,authors:[{name:"Yu-Wei Chao",bold:!1},{name:"Yunfan Liu",bold:!1},{name:"Xieyang Liu",bold:!0},{name:"Huayi Zeng",bold:!1},{name:"Jia Deng",bold:!1}],abstract:"In this paper we study the problem of detecting human-object interactions (HOI) in static images, defined as predicting a human and an object bounding box with an interaction class label that connects them. HOI detection is a fundamental problem in computer vision as it provides semantic information about the interactions among the detected objects. We introduce HICO-DET, a new large benchmark for HOI detection, by augmenting the current HICO classification benchmark with instance annotations. We propose Human-Object Region-based Convolutional Neural Networks (HO-RCNN), a novel DNN-based framework for HOI detection. At the core of our HO-RCNN is the Interaction Pattern, a novel DNN input that characterizes the spatial relations between two bounding boxes. We validate the effectiveness of our HO-RCNN using HICO-DET. Experiments demonstrate that our HO-RCNN, by exploiting human-object spatial relations through Interaction Patterns, significantly improves the performance of HOI detection over baseline approaches. ",codename:"hico",bibtex:"\n  @INPROCEEDINGS{chao:wacv2018,\n    author = {Yu-Wei Chao and Yunfan Liu and Xieyang Liu and Huayi Zeng and Jia Deng},\n    title = {Learning to Detect Human-Object Interactions},\n    booktitle = {Proceedings of the IEEE Winter Conference on Applications of Computer Vision},\n    year = {2018},\n  }    \n      ",previewImgLink:a,projectPageLink:"http://www-personal.umich.edu/~ywchao/hico/",codebaseLink:"https://github.com/ywchao/ho-rcnn",doi:"http://doi.org/10.1109/WACV.2018.00048",ieeexplore:"https://ieeexplore.ieee.org/document/8354152"},{title:"Supporting Knowledge Acceleration for Programming from a Sensemaking Perspective",type:"workshop",conference:"CHI",conferenceFullName:"Sensemaking Workshop @ The ACM Conference on Human Factors in Computing Systems (CHI)",conferenceTag:"CHI 2018",year:2018,month:4,authors:[{name:"Michael Xieyang Liu",bold:!0},{name:"Shaun Burley",bold:!1},{name:"Emily Deng",bold:!1},{name:"Angelina Zhou",bold:!1},{name:"Aniket Kittur",bold:!1},{name:"Brad A. Myers",bold:!1}],abstract:"Programmers spend a signiﬁcant proportion of their time searching for and making sense of complex information. However, they often lack effective tools to help them make sense of the information, turn it into knowledge, or share it with their respective communities. In this position paper, we aim to help programmers collect, navigate, and organize knowledge to meet their goals while capturing this knowledge and making it useful for later programmers with similar needs. We describe barriers and challenges to creating this sustainable cycle, and we explore the design space and opportunities for effective tools and systems.",codename:"kap-sensemaking-workshop",previewImgLink:"data:image/png;base64,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"},{title:"UNAKITE: Support Developers for Capturing and Persisting Design Rationales When Solving Problems Using Web Resources",type:"workshop",conference:"VL/HCC",conferenceFullName:"DTSHPS'18 Workshop on Designing Technologies to Support Human Problem Solving",conferenceTag:"VL/HCC 2018",year:2018,month:10,authors:[{name:"Michael Xieyang Liu",bold:!0},{name:"Nathan Hahn",bold:!1},{name:"Angelina Zhou",bold:!1},{name:"Shaun Burley",bold:!1},{name:"Emily Deng",bold:!1},{name:"Aniket Kittur",bold:!1},{name:"Brad A. Myers",bold:!1}],abstract:"UNAKITE is a new system that supports developers in collecting, organizing, consuming, and persisting design rationales while solving problems using web resources. Understanding design rationale has widely been recognized as signiﬁcant for the success of a software engineering project. However, it is currently both time and labor intensive for little immediate payoff for a developer to generate and embed a useful design rationale in their code. Under this cost structure, there is very little effective tool support to help developers keep track of design rationales. UNAKITE addresses this challenge for some design decisions by changing the cost structure: developers are incentivized to make decisions using UNAKITE's collecting and organizing mechanisms as it makes tracking and deciding between alternatives easier than before; the structure thus generated is automatically embedded in the code as the design rationale when the developer copies sample code into their existing code. In a preliminary usability study developers found UNAKITE to be usable for capturing design rationales and effective for interpreting the rationale of others.",codename:"unakite-vlhcc-workshop",previewImgLink:o,shouldShowLocalPaperLink:!0},{title:"An Exploratory Study of Web Foraging to Understand and Support Programming Decisions",type:"workshop",conference:"VL/HCC",conferenceFullName:"IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)",conferenceTag:"VL/HCC 2018",year:2018,month:10,authors:[{name:"Jane Hsieh",bold:!1},{name:"Michael Xieyang Liu",bold:!0},{name:"Brad A. Myers",bold:!1},{name:"Aniket Kittur",bold:!1}],abstract:"Programmers consistently engage in cognitively demanding tasks such as sensemaking and decision-making. During the information-foraging process, programmers are growing more reliant on resources available online since they contain masses of crowdsourced information and are easier to navigate. Content available in questions and answers on Stack Overflow presents a unique platform for studying the types of problems encountered in programming and possible solutions. In addition to classifying these questions, we introduce possible visual representations for organizing the gathered information and propose that such models may help reduce the cost of navigating, understanding and choosing solution alternatives.",codename:"stackoverflow-vlhcc-workshop",previewImgLink:r,shouldShowLocalPaperLink:!0,doi:"https://doi.org/10.1109/VLHCC.2018.8506517",ieeexplore:"https://ieeexplore.ieee.org/document/8506517"},{title:"Popup: Reconstructing 3D Video Using Particle Filtering to Aggregate Crowd Responses",type:"conference",conference:"IUI",conferenceFullName:"ACM International Conference on Intelligent User Interfaces (IUI)",conferenceTag:"IUI 2019",year:2019,month:3,authors:[{name:"Jean Y. Song",bold:!1},{name:"Stephan J. Lemmer",bold:!1},{name:"Michael Xieyang Liu",bold:!0},{name:"Shiyan Yan",bold:!1},{name:"Juho Kim",bold:!1},{name:"Jason J. Corso",bold:!1},{name:"Walter S. Lasecki",bold:!1}],abstract:"Collecting a sufficient amount of 3D training data for autonomous vehicles to handle rare, but critical, traffic events (e.g., collisions) may take decades of deployment. Abundant video data of such events from municipal traffic cameras and video sharing sites (e.g., YouTube) could provide a potential alternative, but generating realistic training data in the form of 3D video reconstructions is a challenging task beyond the current capabilities of computer vision. Crowdsourcing manual annotations of necessary information has the potential to bridge this gap, but the level of accuracy required to attain usable reconstructions makes this a nearly impossible task for non-experts. In this paper, we propose a novel crowd-machine hybrid method that combines annotations from multiple contents by adopting particle filtering as an aggregation technique. Our approach is capable of leveraging temporal dependencies between video frames, enabling more aggressive filtering thresholds for annotations that can help improve the aggregation quality. The proposed method results in a 33% reduction in the relative error of position estimation compared to a state-of-the-art baseline. Moreover, our method enables skip-based (self-filtering) annotation that reduces the total annotation time for hard-to-annotate frames by 16%. Our approach provides a generalizable means of aggregating more accurate crowd responses even in settings where annotation is especially challenging or error-prone.",codename:"av_anno",bibtex:"",previewImgLink:i,shouldShowLocalPaperLink:!0,doi:"https://doi.org/10.1145/3301275.3302305",acmdl:"https://dl.acm.org/citation.cfm?id=3302305",conferenceTalkVideo:"https://youtu.be/hn0r9Eb9_rQ"},{title:"Unakite: Scaffolding Developers’ Decision-Making Using the Web",type:"conference",conference:"UIST",conferenceFullName:"ACM Symposium on User Interface Software and Technology (UIST)",conferenceTag:"UIST 2019",year:2019,month:10,authors:[{name:"Michael Xieyang Liu",bold:!0},{name:"Jane Hsieh",bold:!1},{name:"Nathan Hahn",bold:!1},{name:"Angelina Zhou",bold:!1},{name:"Emily Deng",bold:!1},{name:"Shaun Burley",bold:!1},{name:"Cynthia Taylor",bold:!1},{name:"Aniket Kittur",bold:!1},{name:"Brad A. Myers",bold:!1}],abstract:"Developers spend a significant portion of their time searching for solutions and methods online. While numerous tools have been developed to support this exploratory process, in many cases the answers to developers' questions involve trade-offs among multiple valid options and not just a single solution. Through interviews, we discovered that developers express a desire for help with decision-making and understanding trade-offs. Through an analysis of Stack Overflow posts, we observed that many answers describe such trade-offs. These findings suggest that tools designed to help a developer capture information and make decisions about trade-offs can provide crucial benefits for both the developers and others who want to understand their design rationale. In this work, we probe this hypothesis with a prototype system named Unakite that collects, organizes, and keeps track of information about trade-offs and builds a comparison table, which can be saved as a design rationale for later use. Our evaluation results show that Unakite reduces the cost of capturing tradeoff-related information by 45%, and that the resulting comparison table speeds up a subsequent developer's ability to understand the trade-offs by about a factor of three.",codename:"unakite-uist-2019",bibtex:"",previewImgLink:s,shouldShowLocalPaperLink:!0,doi:"https://dx.doi.org/10.1145/3332165.3347908",acmdl:"https://dx.doi.org/10.1145/3332165.3347908",acmdl_available:!0,award:{honorableMention:!0},conferenceTalkVideo:"https://youtu.be/UMQ-kWgmbQ4"},{title:"An open repository of real-time COVID-19 indicators",type:"conference",conference:"PNAS",conferenceFullName:"Proceedings of the National Academy of Sciences (PNAS)",conferenceTag:"PNAS 2021",year:2021,month:9,authors:[{name:"Alex Reinhart",bold:!1},{name:"Logan Brooks",bold:!1},{name:"Maria Jahja",bold:!1},{name:"Aaron Rumack",bold:!1},{name:"Jingjing Tang",bold:!1},{name:"[et al., including Michael Xieyang Liu]",bold:!0}],abstract:"The COVID-19 pandemic presented enormous data challenges in the United States. Policy makers, epidemiological modelers, and health researchers all require up-to-date data on the pandemic and relevant public behavior, ideally at fine spatial and temporal resolution. The COVIDcast API is our attempt to fill this need: Operational since April 2020, it provides open access to both traditional public health surveillance signals (cases, deaths, and hospitalizations) and many auxiliary indicators of COVID-19 activity, such as signals extracted from deidentified medical claims data, massive online surveys, cell phone mobility data, and internet search trends. These are available at a fine geographic resolution (mostly at the county level) and are updated daily. The COVIDcast API also tracks all revisions to historical data, allowing modelers to account for the frequent revisions and backfill that are common for many public health data sources. All of the data are available in a common format through the API and accompanying R and Python software packages. This paper describes the data sources and signals, and provides examples demonstrating that the auxiliary signals in the COVIDcast API present information relevant to tracking COVID activity, augmenting traditional public health reporting and empowering research and decision-making.",codename:"covidcast-pnas-2021",bibtex:"",previewImgLink:l,shouldShowLocalPaperLink:!0,doi:"https://doi.org/10.1073/pnas.2111452118",pnasdl:"https://doi.org/10.1073/pnas.2111452118",pnasdl_available:!0,award:{honorableMention:!1},website:"https://delphi.cmu.edu/covidcast/"},{title:"Tabs.do: Task-Centric Browser Tab Management",type:"conference",conference:"UIST",conferenceFullName:"ACM Symposium on User Interface Software and Technology (UIST)",conferenceTag:"UIST 2021",year:2021,month:10,authors:[{name:"Joseph Chee Chang",bold:!1},{name:"Yongsung Kim",bold:!1},{name:"Victor Miller",bold:!1},{name:"Michael Xieyang Liu",bold:!0},{name:"Brad A. Myers",bold:!1},{name:"Aniket Kittur",bold:!1}],abstract:"Despite the increasing complexity and scale of people's online activities, browser interfaces have stayed largely the same since tabs were introduced in major browsers nearly 20 years ago. The gap between simple tab-based browser interfaces and the complexity of users' tasks can lead to serious adverse effects – commonly referred to as \"tab overload.\" This paper introduces a Chrome extension called Tabs.do, which explores bringing a task-centric approach to the browser, helping users to group their tabs into tasks and then organize, prioritize, and switch between those tasks fluidly. To lower the cost of importing, Tabs.do uses machine learning to make intelligent suggestions for grouping users' open tabs into task bundles by exploiting behavioral and semantic features. We conducted a field deployment study where participants used Tabs.do with their real-life tasks in the wild, and showed that Tabs.do can decrease tab clutter, enabled users to create rich task structures with lightweight interactions, and allowed participants to context-switch among tasks more efficiently.",codename:"tabsdo-uist-2021",bibtex:"",previewImgLink:u,shouldShowLocalPaperLink:!0,doi:"https://doi.org/10.1145/3472749.3474777",acmdl:"https://doi.org/10.1145/3472749.3474777",acmdl_available:!0,award:{honorableMention:!1},conferenceTalkVideo:"https://youtu.be/UrXMVkqfYbg"},{title:"To Reuse or Not To Reuse? A Framework and System for Evaluating Summarized Knowledge",type:"conference",conference:"CSCW",conferenceFullName:"ACM Conference on Computer Supported Cooperative Work and Social Computing (CSCW)",conferenceTag:"CSCW 2021",year:2021,month:10,authors:[{name:"Michael Xieyang Liu",bold:!0},{name:"Aniket Kittur",bold:!1},{name:"Brad A. Myers",bold:!1}],abstract:"As the amount of information online continues to grow, a correspondingly important opportunity is for individuals to reuse knowledge which has been summarized by others rather than starting from scratch. However, appropriate reuse requires judging the relevance, trustworthiness, and thoroughness of others' knowledge in relation to an individual's goals and context. In this work, we explore augmenting judgements of the appropriateness of reusing knowledge in the domain of programming, specifically of reusing artifacts that result from other developers' searching and decision making. Through an analysis of prior research on sensemaking and trust, along with new interviews with developers, we synthesized a framework for reuse judgements. The interviews also validated that developers express a desire for help with judging whether to reuse an existing decision. From this framework, we developed a set of techniques for capturing the initial decision maker's behavior and visualizing signals calculated based on the behavior, to facilitate subsequent consumers' reuse decisions, instantiated in a prototype system called Strata. Results of a user study suggest that the system significantly improves the accuracy, depth, and speed of reusing decisions. These results have implications for systems involving user-generated content in which other users need to evaluate the relevance and trustworthiness of that content.",codename:"strata-cscw-2021",bibtex:"",previewImgLink:c,shouldShowLocalPaperLink:!0,doi:"https://doi.org/10.1145/3449240",acmdl:"https://doi.org/10.1145/3449240",arxiv:"https://arxiv.org/abs/2102.06231",cmuSCSMedia:"https://www.cs.cmu.edu/news/2021/reuse-content-tool",acmdl_available:!0,award:{bestPaper:!0},conferenceTalkVideo:"https://youtu.be/NuL-jtf710E"},{title:"Understanding How Programmers Can Use Annotations on Documentation",type:"conference",conference:"CHI",conferenceFullName:"ACM CHI Conference on Human Factors in Computing Systems (CHI)",conferenceTag:"CHI 2022",year:2022,month:4,authors:[{name:"Amber Horvath",bold:!1},{name:"Michael Xieyang Liu",bold:!0},{name:"River Hendriksen",bold:!1},{name:"Connor Shannon",bold:!1},{name:"Emma Paterson",bold:!1},{name:"Kazi Jawad",bold:!1},{name:"Andrew Macvean",bold:!1},{name:"Brad A. Myers",bold:!1}],abstract:"Modern software development requires developers to find and effectively utilize new APIs and their documentation, but documentation has many well-known issues. Despite this, developers eventually overcome these issues but have no way of sharing what they learned. We investigate sharing this documentation-specific information through annotations, which have advantages over developer forums as the information is contextualized, not disruptive, and is short, thus easy to author. Developers can also author annotations to support their own comprehension. In order to support the documentation usage behaviors we found, we built the Adamite annotation tool, which provides features such as multiple anchors, annotation types, and pinning. In our user study, we found that developers are able to create annotations that are useful to themselves and are able to utilize annotations created by other developers when learning a new API, with readers of the annotations completing 67% more of the task, on average, than the baseline.",codename:"adamite-chi-2022",bibtex:"",previewImgLink:p,shouldShowLocalPaperLink:!0,doi:"https://doi.org/10.1145/3491102.3502095",acmdl:"https://doi.org/10.1145/3491102.3502095",arxiv:"https://arxiv.org/abs/2111.08684",acmdl_available:!0,conferenceTalkVideo:"https://youtu.be/NW6A2hoFh5E"},{title:"Crystalline: Lowering the Cost for Developers to Collect and Organize Information for Decision Making",type:"conference",conference:"CHI",conferenceFullName:"ACM CHI Conference on Human Factors in Computing Systems (CHI)",conferenceTag:"CHI 2022",year:2022,month:4,authors:[{name:"Michael Xieyang Liu",bold:!0},{name:"Aniket Kittur",bold:!1},{name:"Brad A. Myers",bold:!1}],abstract:"Developers perform online sensemaking on a daily basis, such as researching and choosing libraries and APIs. Prior research has introduced tools that help developers capture information from various sources and organize it into structures useful for subsequent decision-making. However, it remains a laborious process for developers to manually identify and clip content, maintaining its provenance and synthesizing it with other content. In this work, we introduce a new system called Crystalline that attempts to automatically collect and organize information into tabular structures as the user searches and browses the web. It leverages natural language processing to automatically group similar criteria together to reduce clutter as well as passive behavioral signals such as mouse movement and dwell time to infer what information to collect and how to visualize and prioritize it. Our user study suggests that developers are able to create comparison tables about 20% faster with a 60% reduction in operational cost without sacrificing the quality of the tables.",codename:"crystalline-chi-2022",bibtex:"",previewImgLink:d,shouldShowLocalPaperLink:!0,doi:"https://doi.org/10.1145/3491102.3501968",acmdl:"https://doi.org/10.1145/3491102.3501968",arxiv:"https://arxiv.org/abs/2202.02175",acmdl_available:!0,conferenceTalkVideo:"https://youtu.be/VO-osTVGuJs"},{title:"Freedom to Choose: Understanding Input Modality Preferences of People with Upper-body Motor Impairments for Activities of Daily Living",type:"conference",conference:"ASSETS",conferenceFullName:"ACM SIGACCESS Conference on Computers and Accessibility",conferenceTag:"ASSETS 2022",year:2022,month:10,authors:[{name:"Franklin Mingzhe Li",bold:!1},{name:"Michael Xieyang Liu",bold:!0},{name:"Yang Zhang",bold:!1},{name:"Patrick Carrington",bold:!1}],abstract:"Many people with upper-body motor impairments encounter challenges while performing Activities of Daily Living (ADLs) and Instrumental Activities of Daily Living (IADLs), such as toileting, grooming, and managing finances, which have impacts on their Quality of Life (QOL). Although existing assistive technologies enable people with upper-body motor impairments to use different input modalities to interact with computing devices independently (e.g., using voice to interact with a computer), many people still require Personal Care Assistants (PCAs) to perform ADLs. Multimodal input has the potential to enable users to perform ADLs without human assistance. We conducted 12 semi-structured interviews with people who have upper-body motor impairments to capture their existing practices and challenges of performing ADLs, identify opportunities to expand the input possibilities for assistive devices, and understand user preferences for multimodal interaction during everyday tasks. Finally, we discuss implications for the design and use of multimodal input solutions to support user independence and collaborative experiences when performing daily living tasks.",codename:"multimodal-assets-2022",bibtex:"",previewImgLink:h,shouldShowLocalPaperLink:!0,doi:"https://doi.org/10.1145/3517428.3544814",acmdl:"https://doi.org/10.1145/3517428.3544814",arxiv:"https://arxiv.org/abs/2207.04344",acmdl_available:!0},{title:"Wigglite: Low-cost Information Collection and Triage",type:"conference",conference:"UIST",conferenceFullName:"ACM Symposium on User Interface Software and Technology (UIST)",conferenceTag:"UIST 2022",year:2022,month:10,authors:[{name:"Michael Xieyang Liu",bold:!0},{name:"Andrew Kuznetsov",bold:!1},{name:"Yongsung Kim",bold:!1},{name:"Joseph Chee Chang",bold:!1},{name:"Aniket Kittur",bold:!1},{name:"Brad A. Myers",bold:!1}],abstract:"\n      Consumers conducting comparison shopping, researchers making sense of competitive space, and developers looking for code snippets online all face the challenge of capturing the information they find for later use without interrupting their current flow. In addition, during many learning and exploration tasks, people need to externalize their mental context, such as estimating how urgent a topic is to follow up on, or rating a piece of evidence as a “pro” or “con,” which helps scaffold subsequent deeper exploration. However, current approaches incur a high cost, often requiring users to select, copy, context switch, paste, and annotate information in a separate document without offering specific affordances that capture their mental context. In this work, we explore a new interaction technique called “wiggling,” which can be used to fluidly collect, organize, and rate information during early sensemaking stages with a single gesture. Wiggling involves rapid back-and-forth movements of a pointer or up-and-down scrolling on a smartphone, which can indicate the information to be collected and its valence, using a single, light-weight gesture that does not interfere with other interactions that are already available. Through implementation and user evaluation, we found that wiggling helped participants accurately collect information and encode their mental context with a 58% reduction in operational cost while being 24% faster compared to a common baseline.\n      ",codename:"wigglite-uist-2022",bibtex:"",previewImgLink:f,shouldShowLocalPaperLink:!0,doi:"https://doi.org/10.1145/3526113.3545661",acmdl:"https://doi.org/10.1145/3526113.3545661",arxiv:"https://arxiv.org/abs/2208.00496",acmdl_available:!0,conferenceTalkVideo:"https://youtu.be/_MH81Zuyj64"},{title:"Multidirectional Gesturing for OnDisplay Item Identification and/or Further Action Control",type:"patent",conferenceFullName:"US Patent PCT/US2022/043604 (submitted)",conferenceTag:"US Patent PCT/US2022/043604",year:2022,month:9,authors:[{name:"Aniket Kittur",bold:!1},{name:"Brad A. Myers",bold:!1},{name:"Michael Xieyang Liu",bold:!0}],abstract:"",codename:"us-patent-wiggle-2022",bibtex:"",shouldShowLocalPaperLink:!1,acmdl_available:!1},{title:"Generation of Interactive Utterances of Code Tasks",type:"patent",conferenceFullName:"US Patent (submitted)",year:2022,month:9,authors:[{name:"Ben Zorn",bold:!1},{name:"Carina Negreanu",bold:!1},{name:"Advait Sarkar",bold:!1},{name:"Andrew Gordon",bold:!1},{name:"Jack Williams",bold:!1},{name:"Michael Xieyang Liu",bold:!0},{name:"Neil Toronto",bold:!1},{name:"Sruti Srinivasa Ragavan",bold:!1}],abstract:"",codename:"us-patent-codeexcel-2022",bibtex:"",shouldShowLocalPaperLink:!1,acmdl_available:!1},{title:"Facilitating Counselor Reflective Learning With a Real-time Annotation Tool",type:"conference",conference:"CHI",conferenceFullName:"ACM CHI Conference on Human Factors in Computing Systems (CHI)",conferenceTag:"CHI 2023",year:2023,month:4,authors:[{name:"Tianying Chen",bold:!1},{name:"Michael Xieyang Liu",bold:!0},{name:"Emily Ding",bold:!1},{name:"Emma O’Neil",bold:!1},{name:"Mansi Agarwal",bold:!1},{name:"Robert E. Kraut",bold:!1},{name:"Laura Dabbish",bold:!1}],abstract:'Experiential training, where mental health professionals practice their learned skills, remains the most costly component of therapeutic training. We introduce Pinion, a tool that supports experiential learning of counseling skills through interactive role-play as client and counselors. In Pinion counselors annotate, or "pin" the important moments in their role-play sessions in real-time. The pins are then used post-session to facilitate a reflective learning process. We discuss the design and qualitative evaluation of Pinion with a set of healthcare professionals learning MI. Our evaluation suggests that Pinion helped users develop empathy, be more aware of their skill usage, guaranteed immediate and targeted feedback, and helped users correct the misconceptions about their performances. We discuss implications for the design of experiential training tools for learning counseling skills.',codename:"pinmi-chi-2023",bibtex:"",previewImgLink:m,shouldShowLocalPaperLink:!0,doi:"https://doi.org/10.1145/3544548.3581551",acmdl:"https://doi.org/10.1145/3544548.3581551",acmdl_available:!0,showOnWebCV:!0,showInResearchPage:!0},{title:'"What It Wants Me To Say": Bridging the Abstraction Gap Between End-User Programmers and Code-Generating Large Language Models',type:"conference",conference:"CHI",conferenceFullName:"ACM CHI Conference on Human Factors in Computing Systems (CHI)",conferenceTag:"CHI 2023",year:2023,month:4,authors:[{name:"Michael Xieyang Liu",bold:!0},{name:"Advait Sarkar",bold:!1},{name:"Carina Negreanu",bold:!1},{name:"Ben Zorn",bold:!1},{name:"Jack Williams",bold:!1},{name:"Neil Toronto",bold:!1},{name:"Andrew D. Gordon",bold:!1}],abstract:"Code-generating large language models translate natural language into code. However, only a small portion of the infinite space of naturalistic utterances is effective at guiding code generation. For non-expert end-user programmers, learning this is the challenge of abstraction matching. We examine this challenge in the specific context of data analysis in spreadsheets, in a system that maps the users natural language query to Python code using the Codex generator, executes the code, and shows the result. We propose grounded abstraction matching, which bridges the abstraction gap by translating the code back into a systematic and predictable naturalistic utterance. In a between-subjects, think-aloud study (n=24), we compare grounded abstraction matching to an ungrounded alternative based on previously established query framing principles. We find that the grounded approach improves end-users' understanding of the scope and capabilities of the code-generating model, and the kind of language needed to use it effectively.",codename:"llmgam-chi-2023",bibtex:"",previewImgLink:g,shouldShowLocalPaperLink:!0,doi:"https://doi.org/10.1145/3544548.3580817",acmdl:"https://doi.org/10.1145/3544548.3580817",arxiv:"https://arxiv.org/abs/2304.06597",acmdl_available:!0,conferenceTalkVideo:"https://youtu.be/w9PXnm_outI",msrblog:"https://www.microsoft.com/en-us/research/blog/advancing-transparency-updates-on-responsible-ai-research/",msrblogTitle:"Advancing transparency: Updates on responsible AI research - Microsoft Research Blog",showOnWebCV:!0,showInResearchPage:!0,award:{honorableMention:!0}},{title:"A Contextual Inquiry of People with Vision Impairments in Cooking",type:"conference",conference:"CHI",conferenceFullName:"ACM CHI Conference on Human Factors in Computing Systems (CHI)",conferenceTag:"CHI 2024",year:2024,month:5,authors:[{name:"Franklin Mingzhe Li",bold:!1},{name:"Michael Xieyang Liu",bold:!0},{name:"Shaun K. Kane",bold:!1},{name:"Patrick Carrington",bold:!1}],abstract:"Individuals with vision impairments employ a variety of strategies for object identification, such as pans or soy sauce, in the culinary process. In addition, they often rely on contextual details about objects, such as location, orientation, and current status, to autonomously execute cooking activities. To understand how people with vision impairments collect and use the contextual information of objects while cooking, we conducted a contextual inquiry study with 12 participants in their own kitchens. This research aims to analyze object interaction dynamics in culinary practices to enhance assistive vision technologies for visually impaired cooks. We outline eight different types of contextual information and the strategies that blind cooks currently use to access the information while preparing meals. Further, we discuss preferences for communicating contextual information about kitchen objects as well as considerations for the deployment of AI-powered assistive technologies.",codename:"cooking-chi-2024",bibtex:"@inproceedings{10.1145/3613904.3642233,\nauthor = {Li, Franklin Mingzhe and Liu, Michael Xieyang and Kane, Shaun K. and Carrington, Patrick},\ntitle = {A Contextual Inquiry of People with Vision Impairments in Cooking},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642233},\ndoi = {10.1145/3613904.3642233},\nabstract = {Individuals with vision impairments employ a variety of strategies for object identification, such as pans or soy sauce, in the culinary process. In addition, they often rely on contextual details about objects, such as location, orientation, and current status, to autonomously execute cooking activities. To understand how people with vision impairments collect and use the contextual information of objects while cooking, we conducted a contextual inquiry study with 12 participants in their own kitchens. This research aims to analyze object interaction dynamics in culinary practices to enhance assistive vision technologies for visually impaired cooks. We outline eight different types of contextual information and the strategies that blind cooks currently use to access the information while preparing meals. Further, we discuss preferences for communicating contextual information about kitchen objects as well as considerations for the deployment of AI-powered assistive technologies.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {38},\nnumpages = {14},\nkeywords = {Accessibility, Assistive technology, Blind, Contextual Inquiry, Cooking, People with Vision Impairments},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",previewImgLink:y,shouldShowLocalPaperLink:!0,doi:"https://doi.org/10.1145/3613904.3642233",acmdl:"https://doi.org/10.1145/3613904.3642233",acmdl_available:!0,arxiv:"https://arxiv.org/abs/2402.15108",conferenceTalkVideo:"https://youtu.be/f7yopOQOtL8",showOnWebCV:!0,showInResearchPage:!0},{title:"Selenite: Scaffolding Online Sensemaking with Comprehensive Overviews Elicited from Large Language Models",type:"conference",conference:"CHI",conferenceFullName:"ACM CHI Conference on Human Factors in Computing Systems (CHI)",conferenceTag:"CHI 2024",year:2024,month:5,authors:[{name:"Michael Xieyang Liu",bold:!0},{name:"Sherry Tongshuang Wu",bold:!1},{name:"Tianying Chen",bold:!1},{name:"Franklin Mingzhe Li",bold:!1},{name:"Aniket Kittur",bold:!1},{name:"Brad A. Myers",bold:!1}],abstract:"Sensemaking in unfamiliar domains can be challenging, demanding considerable user effort to compare different options with respect to various criteria. Prior research and our formative study found that people would benefit from reading an overview of an information space upfront, including the criteria others previously found useful. However, existing sensemaking tools struggle with the \"cold-start\" problem -- not only requiring significant input from previous users to generate and share these overviews, but also that such overviews may turn out to be biased and incomplete. In this work, we introduce a novel system, Selenite, which leverages Large Language Models (LLMs) as reasoning machines and knowledge retrievers to automatically produce a comprehensive overview of options and criteria to jumpstart users' sensemaking processes. Subsequently, Selenite also adapts as people use it, helping users find, read, and navigate unfamiliar information in a systematic yet personalized manner. Through three studies, we found that Selenite produced accurate and high-quality overviews reliably, significantly accelerated users' information processing, and effectively improved their overall comprehension and sensemaking experience.",codename:"selenite-chi-2024",bibtex:"@inproceedings{10.1145/3613904.3642149,\nauthor = {Liu, Michael Xieyang and Wu, Tongshuang and Chen, Tianying and Li, Franklin Mingzhe and Kittur, Aniket and Myers, Brad A},\ntitle = {Selenite: Scaffolding Online Sensemaking with Comprehensive Overviews Elicited from Large Language Models},\nyear = {2024},\nisbn = {9798400703300},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613904.3642149},\ndoi = {10.1145/3613904.3642149},\nabstract = {Sensemaking in unfamiliar domains can be challenging, demanding considerable user effort to compare different options with respect to various criteria. Prior research and our formative study found that people would benefit from reading an overview of an information space upfront, including the criteria others previously found useful. However, existing sensemaking tools struggle with the “cold-start” problem — it not only requires significant input from previous users to generate and share these overviews, but such overviews may also turn out to be biased and incomplete. In this work, we introduce a novel system, Selenite, which leverages Large Language Models (LLMs) as reasoning machines and knowledge retrievers to automatically produce a comprehensive overview of options and criteria to jumpstart users’ sensemaking processes. Subsequently, Selenite also adapts as people use it, helping users find, read, and navigate unfamiliar information in a systematic yet personalized manner. Through three studies, we found that Selenite produced accurate and high-quality overviews reliably, significantly accelerated users’ information processing, and effectively improved their overall comprehension and sensemaking experience.},\nbooktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\narticleno = {837},\nnumpages = {26},\nkeywords = {Human-AI Collaboration, Large Language Models, Natural Language Processing, Sensemaking},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '24}\n}",previewImgLink:w,shouldShowLocalPaperLink:!0,doi:"https://doi.org/10.1145/3613904.3642149",acmdl:"https://doi.org/10.1145/3613904.3642149",arxiv:"https://arxiv.org/abs/2310.02161",acmdl_available:!0,conferenceTalkVideo:"https://youtu.be/S8a0yxU0Me0",cmuSCSMedia:"https://www.cs.cmu.edu/news/2024/selenite-search-engine-tool",showOnWebCV:!0,showInResearchPage:!0},{title:"LLM Comparator: Visual Analytics for Side-by-Side Evaluation of Large Language Models",type:"conference",conference:"CHI",conferenceFullName:"Extended Abstract in ACM CHI Conference on Human Factors in Computing Systems (CHI)",conferenceTag:"CHI 2024",year:2024,month:2,authors:[{name:"Minsuk Kahng",bold:!1},{name:"Ian Tenney",bold:!1},{name:"Mahima Pushkarna",bold:!1},{name:"Michael Xieyang Liu",bold:!0},{name:"James Wexler",bold:!1},{name:"Emily Reif",bold:!1},{name:"Krystal Kallarackal",bold:!1},{name:"Minsuk Chang",bold:!1},{name:"Michael Terry",bold:!1},{name:"Lucas Dixon",bold:!1}],abstract:"Automatic side-by-side evaluation has emerged as a promising approach to evaluating the quality of responses from large language models (LLMs). However, analyzing the results from this evaluation approach raises scalability and interpretability challenges. In this paper, we present LLM Comparator, a novel visual analytics tool for interactively analyzing results from automatic side-by-side evaluation. The tool supports interactive workflows for users to understand when and why a model performs better or worse than a baseline model, and how the responses from two models are qualitatively different. We iteratively designed and developed the tool by closely working with researchers and engineers at a large technology company. This paper details the user challenges we identified, the design and development of the tool, and an observational study with participants who regularly evaluate their models.",codename:"llm-comparator-2024",bibtex:"@inproceedings{10.1145/3613905.3650755,\nauthor = {Kahng, Minsuk and Tenney, Ian and Pushkarna, Mahima and Liu, Michael Xieyang and Wexler, James and Reif, Emily and Kallarackal, Krystal and Chang, Minsuk and Terry, Michael and Dixon, Lucas},\ntitle = {LLM Comparator: Visual Analytics for Side-by-Side Evaluation of Large Language Models},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650755},\ndoi = {10.1145/3613905.3650755},\nabstract = {Automatic side-by-side evaluation has emerged as a promising approach to evaluating the quality of responses from large language models (LLMs). However, analyzing the results from this evaluation approach raises scalability and interpretability challenges. In this paper, we present LLM Comparator, a novel visual analytics tool for interactively analyzing results from automatic side-by-side evaluation. The tool supports interactive workflows for users to understand when and why a model performs better or worse than a baseline model, and how the responses from two models are qualitatively different. We iteratively designed and developed the tool by closely working with researchers and engineers at Google. This paper details the user challenges we identified, the design and development of the tool, and an observational study with participants who regularly evaluate their models.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {216},\nnumpages = {7},\nkeywords = {Visual analytics, generative AI, large language models, machine learning evaluation, side-by-side evaluation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '24}\n}",previewImgLink:b,shouldShowLocalPaperLink:!0,doi:"https://doi.org/10.1145/3613905.3650755",acmdl:"https://doi.org/10.1145/3613905.3650755",arxiv:"https://arxiv.org/abs/2402.10524",acmdl_available:!0,conferenceTalkVideo:"https://youtu.be/UpgoHnXvwC0",blogPost:"https://medium.com/people-ai-research/llm-comparator-a-tool-for-human-driven-llm-evaluation-81292c17f521",showOnWebCV:!0,showInResearchPage:!0},{title:'"We Need Structured Output": Towards User-centered Constraints on Large Language Model Output',type:"conference",conference:"CHI",conferenceFullName:"Extended Abstract in ACM CHI Conference on Human Factors in Computing Systems (CHI)",conferenceTag:"CHI 2024",year:2024,month:3,authors:[{name:"Michael Xieyang Liu",bold:!0},{name:"Frederick Liu",bold:!1},{name:"Alexander J. Fiannaca",bold:!1},{name:"Terry Koo",bold:!1},{name:"Lucas Dixon",bold:!1},{name:"Michael Terry",bold:!1},{name:"Carrie J. Cai",bold:!1}],abstract:"Large language models can produce creative and diverse responses. However, to integrate them into current developer workflows, it is essential to constrain their outputs to follow specific formats or standards. In this work, we surveyed 51 experienced industry professionals to understand the range of scenarios and motivations driving the need for output constraints from a user-centered perspective. We identified 134 concrete use cases for constraints at two levels: low-level, which ensures the output adhere to a structured format and an appropriate length, and high-level, which requires the output to follow semantic and stylistic guidelines without hallucination. Critically, applying output constraints could not only streamline the currently repetitive process of developing, testing, and integrating LLM prompts for developers, but also enhance the user experience of LLM-powered features and applications. We conclude with a discussion on user preferences and needs towards articulating intended constraints for LLMs, alongside an initial design for a constraint prototyping tool.",codename:"llm-constraints-2024",bibtex:'@inproceedings{10.1145/3613905.3650756,\nauthor = {Liu, Michael Xieyang and Liu, Frederick and Fiannaca, Alexander J. and Koo, Terry and Dixon, Lucas and Terry, Michael and Cai, Carrie J.},\ntitle = {"We Need Structured Output": Towards User-centered Constraints on Large Language Model Output},\nyear = {2024},\nisbn = {9798400703317},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3613905.3650756},\ndoi = {10.1145/3613905.3650756},\nabstract = {Large language models can produce creative and diverse responses. However, to integrate them into current developer workflows, it is essential to constrain their outputs to follow specific formats or standards. In this work, we surveyed 51 experienced industry professionals to understand the range of scenarios and motivations driving the need for output constraints from a user-centered perspective. We identified 134 concrete use cases for constraints at two levels: low-level, which ensures the output adhere to a structured format and an appropriate length, and high-level, which requires the output to follow semantic and stylistic guidelines without hallucination. Critically, applying output constraints could not only streamline the currently repetitive process of developing, testing, and integrating LLM prompts for developers, but also enhance the user experience of LLM-powered features and applications. We conclude with a discussion on user preferences and needs towards articulating intended constraints for LLMs, alongside an initial design for a constraint prototyping tool.},\nbooktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {10},\nnumpages = {9},\nkeywords = {Constrained generation, Large language models, Survey},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA \'24}\n}',previewImgLink:v,shouldShowLocalPaperLink:!0,doi:"https://doi.org/10.1145/3613905.3650756",acmdl:"https://doi.org/10.1145/3613905.3650756",arxiv:"https://arxiv.org/abs/2404.07362",acmdl_available:!0,conferenceTalkVideo:"https://youtu.be/JEJpBbk5CAs",showOnWebCV:!0,showInResearchPage:!0},{title:"In Situ AI Prototyping: Infusing Multimodal Prompts into Mobile Settings with MobileMaker",type:"conference",conference:"VL/HCC",conferenceFullName:"IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)",conferenceTag:"VL/HCC 2024",year:2024,month:9,authors:[{name:"Michael Xieyang Liu*",bold:!0},{name:"Savvas Petridis*",bold:!1},{name:"Alexander J. Fiannaca",bold:!1},{name:"Vivian Tsai",bold:!1},{name:"Michael Terry",bold:!1},{name:"Carrie J. Cai",bold:!1}],abstract:"Recent advances in multimodal large language models (LLMs) have made it easier to rapidly prototype AI-powered features, especially for mobile use cases. However, gathering early, mobile-situated user feedback on these AI prototypes remains challenging. The broad scope and flexibility of LLMs means that, for a given use-case-specific prototype, there is a crucial need to understand the wide range of in-the-wild input users are likely to provide and their in-context expectations for the AI's behavior. To explore the concept of in situ AI prototyping and testing, we created MobileMaker: a platform that enables designers to rapidly create and test mobile AI prototypes directly on devices. This tool also enables testers to make on-device, in-the-field revisions of prototypes using natural language. In an exploratory study with 16 participants, we explored how user feedback on prototypes created with MobileMaker compares to that of existing prototyping tools (e.g., Figma, prompt editors). Our findings suggest that MobileMaker prototypes enabled more serendipitous discovery of: model input edge cases, discrepancies between AI's and user's in-context interpretation of the task, and contextual signals missed by the AI. Furthermore, we learned that while the ability to make in-the-wild revisions led users to feel more fulfilled as active participants in the design process, it might also constrain their feedback to the subset of changes perceived as more actionable or implementable by the prototyping tool.",codename:"mobilemaker-vlhcc-2024",bibtex:"@INPROCEEDINGS{10714531,\n  author={Petridis, Savvas and Liu, Michael Xieyang and Fiannaca, Alexander J. and Tsai, Vivian and Terry, Michael and Cai, Carrie J.},\n  booktitle={2024 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)}, \n  title={In Situ AI Prototyping: Infusing Multimodal Prompts into Mobile Settings with MobileMaker}, \n  year={2024},\n  volume={},\n  number={},\n  pages={121-133},\n  keywords={Visualization;Large language models;Image edge detection;Natural languages;Prototypes;Real-time systems;Artificial intelligence;Testing;Context modeling;Prototyping;LLMs;Generative AI;Design},\n  doi={10.1109/VL/HCC60511.2024.00023}}\n",previewImgLink:k,shouldShowLocalPaperLink:!0,doi:"https://doi.org/10.1109/VL/HCC60511.2024.00023",ieeexplore:"https://ieeexplore.ieee.org/document/10714531",arxiv:"https://arxiv.org/abs/2405.03806",showOnWebCV:!0,showInResearchPage:!0},{title:"LLM Comparator: Interactive Analysis of Side-by-Side Evaluation of Large Language Models",type:"conference",conference:"VIS",conferenceFullName:"IEEE Transactions on Visualization and Computer Graphics",conferenceTag:"IEEE VIS 2024",year:2024,month:10,authors:[{name:"Minsuk Kahng",bold:!1},{name:"Ian Tenney",bold:!1},{name:"Mahima Pushkarna",bold:!1},{name:"Michael Xieyang Liu",bold:!0},{name:"James Wexler",bold:!1},{name:"Emily Reif",bold:!1},{name:"Krystal Kallarackal",bold:!1},{name:"Minsuk Chang",bold:!1},{name:"Michael Terry",bold:!1},{name:"Lucas Dixon",bold:!1}],abstract:"Evaluating large language models (LLMs) presents unique challenges. While automatic side-by-side evaluation, also known as LLM-as-a-judge, has become a promising solution, model developers and researchers face difficulties with scalability and interpretability when analyzing these evaluation outcomes. To address these challenges, we introduce LLM Comparator, a new visual analytics tool designed for side-by-side evaluations of LLMs. This tool provides analytical workflows that help users understand when and why one LLM outperforms or underperforms another, and how their responses differ. Through close collaboration with practitioners developing LLMs at Google, we have iteratively designed, developed, and refined the tool. Qualitative feedback from these users highlights that the tool facilitates in-depth analysis of individual examples while enabling users to visually overview and flexibly slice data. This empowers users to identify undesirable patterns, formulate hypotheses about model behavior, and gain insights for model improvement. LLM Comparator has been integrated into Google's LLM evaluation platforms and open-sourced.",codename:"llmcomparator-vis-2024",bibtex:"@ARTICLE{10670495,\n  author={Kahng, Minsuk and Tenney, Ian and Pushkarna, Mahima and Liu, Michael Xieyang and Wexler, James and Reif, Emily and Kallarackal, Krystal and Chang, Minsuk and Terry, Michael and Dixon, Lucas},\n  journal={IEEE Transactions on Visualization and Computer Graphics}, \n  title={LLM Comparator: Interactive Analysis of Side-by-Side Evaluation of Large Language Models}, \n  year={2025},\n  volume={31},\n  number={1},\n  pages={503-513},\n  keywords={Internet;Analytical models;Pipelines;Large language models;Oral communication;Numerical models;Feature extraction;Visual analytics;large language models;model evaluation;responsible AI;machine learning interpretability},\n  doi={10.1109/TVCG.2024.3456354}}\n",previewImgLink:x,shouldShowLocalPaperLink:!0,doi:"https://doi.org/10.1109/TVCG.2024.3456354",ieeexplore:"https://ieeexplore.ieee.org/document/10670495",blogPost:"https://medium.com/people-ai-research/llm-comparator-a-tool-for-human-driven-llm-evaluation-81292c17f521",codebaseLink:"https://github.com/PAIR-code/llm-comparator",showOnWebCV:!0,showInResearchPage:!0},{title:"Gensors: Authoring Personalized Visual Sensors with Multimodal Foundation Models and Reasoning",type:"conference",conference:"IUI",conferenceFullName:"ACM Conference on Intelligent User Interfaces (IUI)",conferenceTag:"IUI 2025",year:2025,month:3,authors:[{name:"Michael Xieyang Liu*",bold:!0},{name:"Savvas Petridis*",bold:!1},{name:"Vivian Tsai",bold:!1},{name:"Alexander J. Fiannaca",bold:!1},{name:"Alex Olwal",bold:!1},{name:"Michael Terry",bold:!1},{name:"Carrie J. Cai",bold:!1}],abstract:'Multimodal large language models (MLLMs), with their expansive world knowledge and reasoning capabilities, present a unique opportunity for end-users to create personalized AI sensors capable of reasoning about complex situations. A user could describe a desired sensing task in natural language (e.g., "alert if my toddler is getting into mischief"), with the MLLM analyzing the camera feed and responding within seconds. In a formative study, we found that users saw substantial value in defining their own sensors, yet struggled to articulate their unique personal requirements and debug the sensors through prompting alone. To address these challenges, we developed Gensors, a system that empowers users to define customized sensors supported by the reasoning capabilities of MLLMs. Gensors 1) assists users in eliciting requirements through both automatically-generated and manually created sensor criteria, 2) facilitates debugging by allowing users to isolate and test individual criteria in parallel, 3) suggests additional criteria based on user-provided images, and 4) proposes test cases to help users "stress test" sensors on potentially unforeseen scenarios. In a user study, participants reported significantly greater sense of control, understanding, and ease of communication when defining sensors using Gensors. Beyond addressing model limitations, Gensors supported users in debugging, eliciting requirements, and expressing unique personal requirements to the sensor through criteria-based reasoning; it also helped uncover users\' "blind spots" by exposing overlooked criteria and revealing unanticipated failure modes. Finally, we discuss how unique characteristics of MLLMs--such as hallucinations and inconsistent responses--can impact the sensor-creation process. These findings contribute to the design of future intelligent sensing systems that are intuitive and customizable by everyday users.',codename:"gensors-iui-2025",bibtex:"@inproceedings{10.1145/3708359.3712085,\nauthor = {Liu, Michael Xieyang and Petridis, Savvas and Tsai, Vivian and Fiannaca, Alexander J. and Olwal, Alex and Terry, Michael and Cai, Carrie J.},\ntitle = {Gensors: Authoring Personalized Visual Sensors with Multimodal Foundation Models and Reasoning},\nyear = {2025},\nisbn = {9798400713064},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3708359.3712085},\ndoi = {10.1145/3708359.3712085},\nabstract = {Multimodal large language models (MLLMs), with their expansive world knowledge and reasoning capabilities, present a unique opportunity for end-users to create personalized AI sensors capable of reasoning about complex situations. A user could describe a desired sensing task in natural language (e.g., “let me know if my toddler is getting into mischief in the living room”), with the MLLM analyzing the camera feed and responding within just seconds. In a formative study, we found that users saw substantial value in defining their own sensors, yet struggled to articulate their unique personal requirements to the model and debug the sensors through prompting alone. To address these challenges, we developed Gensors, a system that empowers users to define customized sensors supported by the reasoning capabilities of MLLMs. Gensors 1) assists users in eliciting requirements through both automatically-generated and manually created sensor criteria, 2) facilitates debugging by allowing users to isolate and test individual criteria in parallel, 3) suggests additional criteria based on user-provided images, and 4) proposes test cases to help users “stress test” sensors on potentially unforeseen scenarios. In a 12-participant user study, users reported significantly greater sense of control, understanding, and ease of communication when defining sensors using Gensors. Beyond addressing model limitations, Gensors supported users in debugging, eliciting requirements, and expressing unique personal requirements to the sensor through criteria-based reasoning; it also helped uncover users’ own “blind spots” by exposing overlooked criteria and revealing unanticipated failure modes. Finally, we describe insights into how unique characteristics of MLLMs–such as hallucinations and inconsistent responses–can impact the sensor-creation process. Together, these findings contribute to the design of future MLLM-powered sensing systems that are intuitive and customizable by everyday users.},\nbooktitle = {Proceedings of the 30th International Conference on Intelligent User Interfaces},\npages = {755–770},\nnumpages = {16},\nkeywords = {Human-AI Interaction, Foundation Models, Intelligent Sensing},\nlocation = {\n},\nseries = {IUI '25}\n}",previewImgLink:C,shouldShowLocalPaperLink:!0,doi:"https://doi.org/10.1145/3708359.3712085",acmdl:"https://doi.org/10.1145/3708359.3712085",arxiv:"https://arxiv.org/abs/2501.15727",acmdl_available:!0,showOnWebCV:!0,showInResearchPage:!0,conferenceTalkVideo:"https://youtu.be/tj5WUjPU488"},{title:"LLM Adoption in Data Curation Workflows: Industry Practices and Insights",type:"conference",conference:"CHI",conferenceFullName:"Extended Abstract in ACM CHI Conference on Human Factors in Computing Systems (CHI)",conferenceTag:"CHI 2025",year:2025,month:4,authors:[{name:"Crystal Qian",bold:!1},{name:"Michael Xieyang Liu",bold:!0},{name:"Emily Reif",bold:!1},{name:"Grady Simon",bold:!1},{name:"Nada Hussein",bold:!1},{name:"Nathan Clement",bold:!1},{name:"James Wexler",bold:!1},{name:"Carrie J. Cai",bold:!1},{name:"Michael Terry",bold:!1},{name:"Minsuk Kahng",bold:!1}],abstract:"As large language models (LLMs) grow more proficient at processing unstructured text data, they offer new opportunities to enhance data curation workflows. This paper presents findings from a user study involving 12 industry practitioners from various roles and organizations across a large technology company (N=12). The study examines their data curation workflows before and after LLM adoption, using two custom design probes that integrate LLMs into existing tools. Our study reveals a shift from heuristics-driven, bottom-up curation to insights-driven, top-down workflows supported by LLMs. To navigate increasingly complex data landscapes, practitioners supplement traditional subject-expert-created “golden datasets” with LLM-generated “silver” datasets and rigorously validated “super golden” datasets curated by diverse experts. This research highlights the transformative potential of LLMs in large-scale analysis of unstructured data and highlights opportunities for further tool development.",codename:"industryllm-chi-2025",bibtex:"@inproceedings{10.1145/3706599.3719677,\nauthor = {Qian, Crystal and Liu, Michael Xieyang and Reif, Emily and Simon, Grady and Hussein, Nada and Clement, Nathan and Wexler, James and Cai, Carrie J and Terry, Michael and Kahng, Minsuk},\ntitle = {LLM Adoption in Data Curation Workflows: Industry Practices and Insights},\nyear = {2025},\nisbn = {9798400713958},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3706599.3719677},\ndoi = {10.1145/3706599.3719677},\nabstract = {As large language models (LLMs) grow more proficient at processing unstructured text data, they offer new opportunities to enhance data curation workflows. This paper presents findings from a user study involving 12 industry practitioners from various roles and organizations across a large technology company (N=12). The study examines their data curation workflows before and after LLM adoption, using two custom design probes that integrate LLMs into existing tools. Our study reveals a shift from heuristics-driven, bottom-up curation to insights-driven, top-down workflows supported by LLMs. To navigate increasingly complex data landscapes, practitioners supplement traditional subject-expert-created “golden datasets” with LLM-generated “silver” datasets and rigorously validated “super golden” datasets curated by diverse experts. This research highlights the transformative potential of LLMs in large-scale analysis of unstructured data and highlights opportunities for further tool development.},\nbooktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},\narticleno = {370},\nnumpages = {10},\nlocation = {\n},\nseries = {CHI EA '25}\n}",previewImgLink:I,shouldShowLocalPaperLink:!0,doi:"https://dl.acm.org/doi/abs/10.1145/3706599.3719677",acmdl:"https://dl.acm.org/doi/abs/10.1145/3706599.3719677",acmdl_available:!0,showOnWebCV:!0,showInResearchPage:!0}]}},4553:function(e,n,t){var a=t(4576),o=t(4984),r={};function i(e,n,t){for(var o=new a(n,t,i),r=0;r<e.length&&!s(e[r],o);r++);if(s(null,o),o.currentItem instanceof Object)o.addReference(o.currentItem);else{var l=function(e){for(var n=0;n<e.length;n++)if(!(e[n+1]&&e[n+1].value instanceof Object))return e[n].value}(o.currentParents);l&&o.addReference(l)}return{value:o.currentItem,key:o.currentKey,references:o.currentReferences,parents:o.currentParents}}function s(e,n){if(null==e)!n.currentItem&&n.options.force&&n.force(n.options.force);else if(e.values)if(n.currentItem){var t=Object.keys(n.currentItem),a=[];t.forEach((function(t){e.deep&&Array.isArray(n.currentItem[t])?n.currentItem[t].forEach((function(e){a.push(e)})):a.push(n.currentItem[t])})),n.setCurrent(t,a)}else n.setCurrent(t,[]);else if(e.get){var o=n.getValue(e.get);if(function(e,n){return e.override&&e.currentItem===e.rootContext&&void 0!==e.override[n]}(n,o))n.setCurrent(o,n.override[o]);else if(n.currentItem||n.options.force&&n.force({}))if(function(e,n){return e instanceof Array&&parseInt(n)!=n}(n.currentItem,o)||e.multiple){a=n.currentItem.map((function(e){return e[o]})).filter(c);a=Array.prototype.concat.apply([],a),n.setCurrent(o,a)}else n.setCurrent(o,n.currentItem[o]);else n.setCurrent(o,null)}else if(e.select)if(Array.isArray(n.currentItem)||n.options.force&&n.force([])){var r=(e.boolean?e.select:[e]).map((function(e){if(":"===e.op){var t=n.getValue(e.select[0]);return{func:function(a){return t&&(a=a[t]),n.getValueFrom(e.select[1],a)},negate:e.negate,booleanOp:e.booleanOp}}var a=n.getValues(e.select);if(!n.options.allowRegexp&&"~"===e.op&&a[1]instanceof RegExp)throw new Error("options.allowRegexp is not enabled.");return{key:a[0],value:a[1],negate:e.negate,booleanOp:e.booleanOp,op:e.op}}));if(e.multiple){t=[];var i=[];n.currentItem.forEach((function(e,n){l(e,r)&&(t.push(n),i.push(e))})),n.setCurrent(t,i)}else n.currentItem.some((function(e,t){if(l(e,r))return n.setCurrent(t,e),!0}))||n.setCurrent(null,null)}else n.setCurrent(null,null);else if(e.root)n.resetCurrent(),e.args&&e.args.length?n.setCurrent(null,n.getValue(e.args[0])):n.setCurrent(null,n.rootContext);else if(e.parent)n.resetCurrent(),n.setCurrent(null,n.options.parent);else if(e.or){if(n.currentItem)return!0;n.resetCurrent(),n.setCurrent(null,n.context)}else if(e.filter){var s=n.getLocal(e.filter)||n.getGlobal(e.filter);if("function"==typeof s){a=n.getValues(e.args||[]);var u=s.apply(n.options,[n.currentItem].concat(a));n.setCurrent(null,u)}else{var d=n.getFilter(e.filter);if("function"==typeof d){a=n.getValues(e.args||[]),u=d.call(n.options,n.currentItem,{args:a,state:n,data:n.rootContext});n.setCurrent(null,u)}}}else if(e.deep)if(n.currentItem){if(0===e.deep.length)return;if(u=n.deepQuery(n.currentItem,e.deep,n.options)){n.setCurrent(u.key,u.value);for(var p=0;p<u.parents.length;p++)n.currentParents.push(u.parents[p])}else n.setCurrent(null,null)}else n.currentItem=null}function l(e,n){for(var t=!1,a=0;a<n.length;a++){var o=n[a],r=!1;o.func?r=o.func(e):"~"===o.op?r=o.value instanceof RegExp?e[o.key]&&!!e[o.key].match(o.value):e[o.key]&&!!~e[o.key].indexOf(o.value):"="===o.op?r=!0===e[o.key]&&"true"===o.value||!1===e[o.key]&&"false"===o.value||e[o.key]==o.value:">"===o.op?r=e[o.key]>o.value:"<"===o.op?r=e[o.key]<o.value:">="===o.op?r=e[o.key]>=o.value:"<="===o.op&&(r=e[o.key]<=o.value),o.negate&&(r=!r),t="&"===o.booleanOp?t&&r:"|"===o.booleanOp&&t||r}return t}function c(e){return void 0!==e}e.exports=function(e,n){var t=n&&n.params||null;return Array.isArray(e)&&(t=e.slice(1),e=e[0]),r[e]||(r[e]=o(e,!0)),i(r[e],n,t)},e.exports.lastParent=function(e){var n=e.parents[e.parents.length-1];return n?n.value:null}},3559:function(e){e.exports=function(e,n,t){var a=t&&t.max||1/0,o=t&&t.includeDelimiters||!1,r=0,i=0,s=[],l=[];e.replace(/([\[\(\{])|([\]\)\}])/g,(function(e,n,t,a){n?(0===r&&l.push([i,a]),r+=1):t&&0===(r-=1)&&(i=a+e.length)})),0===r&&i<e.length&&l.push([i,e.length]);i=0;for(var c=0;c<l.length&&a>0;c++)for(var u=l[c][0],d=n.exec(e.slice(u,l[c][1]));d&&a>1;u+=d.index+d[0].length,i=u,d=n.exec(e.slice(u,l[c][1])))s.push(e.slice(i,d.index+u)),o&&s.push(d[0]),a-=1;i<e.length&&s.push(e.slice(i));return s}},4576:function(e){function n(e,n,a){e=e||{},this.handleQuery=a,this.options=e,this.locals=this.options.locals||{},this.globals=this.options.globals||{},this.rootContext=t(e.data,e.rootContext,e.context,e.source),this.parent=e.parent,this.override=e.override,this.filters=e.filters||{},this.params=n||e.params||[],this.context=t(e.currentItem,e.context,e.source),this.currentItem=t(this.context,e.rootContext,e.data),this.currentKey=null,this.currentReferences=[],this.currentParents=[]}function t(e){for(var n=0;n<arguments.length;n++)if(null!=arguments[n])return arguments[n]}function a(e){var n={};if(e)for(var t in e)t in e&&(n[t]=e[t]);return n}e.exports=n,n.prototype={setCurrent:function(e,n){(this.currentItem||this.currentKey||this.currentParents.length>0)&&this.currentParents.push({key:this.currentKey,value:this.currentItem}),this.currentItem=n,this.currentKey=e},resetCurrent:function(){this.currentItem=null,this.currentKey=null,this.currentParents=[]},force:function(e){var n=this.currentParents[this.currentParents.length-1];return!this.currentItem&&n&&null!=this.currentKey&&(this.currentItem=e||{},n.value[this.currentKey]=this.currentItem),!!this.currentItem},getLocal:function(e){if(~e.indexOf("/")){for(var n=null,t=e.split("/"),a=0;a<t.length;a++){var o=t[a];0==a?n=this.locals[o]:n&&n[o]&&(n=n[o])}return n}return this.locals[e]},getGlobal:function(e){if(~e.indexOf("/")){for(var n=null,t=e.split("/"),a=0;a<t.length;a++){var o=t[a];0==a?n=this.globals[o]:n&&n[o]&&(n=n[o])}return n}return this.globals[e]},getFilter:function(e){if(~e.indexOf("/")){for(var n=null,t=e.split("/"),a=0;a<t.length;a++){var o=t[a];0==a?n=this.filters[o]:n&&n[o]&&(n=n[o])}return n}return this.filters[e]},addReferences:function(e){e&&e.forEach(this.addReference,this)},addReference:function(e){e instanceof Object&&!~this.currentReferences.indexOf(e)&&this.currentReferences.push(e)},getValues:function(e,n){return e.map(this.getValue,this)},getValue:function(e){return this.getValueFrom(e,null)},getValueFrom:function(e,n){if(null!=e._param)return this.params[e._param];if(e._sub){var t=a(this.options);t.force=null,t.currentItem=n;var o=this.handleQuery(e._sub,t,this.params);return this.addReferences(o.references),o.value}return e},deepQuery:function(e,n,t,o){Object.keys(e);for(var r in e)if(r in e){(t=a(this.options)).currentItem=e[r];var i=this.handleQuery(n,t,this.params);if(i.value)return i}return null}}},4984:function(e,n,t){var a=t(3559);function o(n){var t=a(n,/(!)?(=|~|\:|<=|>=|<|>)/,{max:2,includeDelimiters:!0});if(3===t.length){var o="!"===t[1].charAt(0),i=r(t[0].trim()),s={negate:o,op:o?t[1].slice(1):t[1]};if(":"===s.op)s.select=[i,{_sub:e.exports(":"+t[2].trim())}];else if("~"===s.op){var l=r(t[2].trim());if("string"==typeof l){var c=t[2].trim().match(/^\/(.*)\/([a-z]?)$/);s.select=c?[i,new RegExp(c[1],c[2])]:[i,l]}else s.select=[i,l]}else s.select=[i,r(t[2].trim())];return s}}function r(n){if("{"===(a=n).charAt(0)&&"}"===a.charAt(a.length-1)){var t=n.slice(1,-1);return{_sub:e.exports(t)}}return function(e){if("?"===e.charAt(0)){var n=parseInt(e.slice(1));return isNaN(n)?e:{_param:n}}return e}(n);var a}e.exports=function(e,n){if(!e)return[];var t,i=[],s=e.charAt(0),l=0,c=0,u=0,d=0,p=0,h="get",f=null;n&&(e=function(e){var n=0;return e.replace(/\?/g,(function(e){return e+n++}))}(e));var g={".":{mode:"get"},":":{mode:"filter"},"|":{handle:"or"},"[":{open:"select"},"]":{close:"select"},"{":{open:"meta"},"}":{close:"meta"},"(":{open:"args"},")":{close:"args"}};function m(e){f?f.push(e):i.push(e)}var b={get:function(e){var n="string"==typeof e?e.trim():null;n&&m({get:n})},select:function(e){if(e)m(function(e){if("*"===e)return{values:!0};if("**"===e)return{values:!0,deep:!0};var n=!1;"*"===e.charAt(0)&&(n=!0,e=e.slice(1));var t=a(e,/&|\|/,{includeDelimiters:!0});if(t.length>1){for(var i=[o(t[0].trim())],s=1;s<t.length;s+=2){var l=o(t[s+1].trim());l&&(l.booleanOp=t[s],i.push(l))}return{multiple:n,boolean:!0,select:i}}return(i=o(e.trim()))?(n&&(i.multiple=!0),i):{get:r(e.trim())}}(e));else{var n={deep:[]};i.push(n),f=n.deep}},filter:function(e){e&&m({filter:e.trim()})},or:function(){f=null,i.push({or:!0}),u=y+1},args:function(e){var n,t=","===(n=e)?[","]:a(n,/,/).map((function(e){return r(e.trim())}));i[i.length-1].args=t}};function v(){var n=e.slice(l,c);b[h]&&b[h](n),h="get",l=c+1}for(var y=0;y<e.length;y++){t,t=s,s=e.charAt(y+1),0===(d=y-u)&&":"!==t&&"."!==t&&i.push({root:!0}),0===d&&"."===t&&"."===s&&i.push({parent:!0});var w=g[t];w&&(0===p&&(w.mode||w.open)&&(v(),h=w.mode||w.open),0===p&&w.handle&&(v(),b[w.handle]()),w.open?p+=1:w.close&&(p-=1),0===p&&w.close&&v()),c=y+1}return v(),i}},8552:function(e,n,t){var a=t(852)(t(5639),"DataView");e.exports=a},1989:function(e,n,t){var a=t(1789),o=t(401),r=t(7667),i=t(1327),s=t(1866);function l(e){var n=-1,t=null==e?0:e.length;for(this.clear();++n<t;){var a=e[n];this.set(a[0],a[1])}}l.prototype.clear=a,l.prototype.delete=o,l.prototype.get=r,l.prototype.has=i,l.prototype.set=s,e.exports=l},8407:function(e,n,t){var a=t(7040),o=t(4125),r=t(2117),i=t(7518),s=t(4705);function l(e){var n=-1,t=null==e?0:e.length;for(this.clear();++n<t;){var a=e[n];this.set(a[0],a[1])}}l.prototype.clear=a,l.prototype.delete=o,l.prototype.get=r,l.prototype.has=i,l.prototype.set=s,e.exports=l},7071:function(e,n,t){var a=t(852)(t(5639),"Map");e.exports=a},3369:function(e,n,t){var a=t(4785),o=t(1285),r=t(6e3),i=t(9916),s=t(5265);function l(e){var n=-1,t=null==e?0:e.length;for(this.clear();++n<t;){var a=e[n];this.set(a[0],a[1])}}l.prototype.clear=a,l.prototype.delete=o,l.prototype.get=r,l.prototype.has=i,l.prototype.set=s,e.exports=l},3818:function(e,n,t){var a=t(852)(t(5639),"Promise");e.exports=a},8525:function(e,n,t){var a=t(852)(t(5639),"Set");e.exports=a},8668:function(e,n,t){var a=t(3369),o=t(619),r=t(2385);function i(e){var n=-1,t=null==e?0:e.length;for(this.__data__=new a;++n<t;)this.add(e[n])}i.prototype.add=i.prototype.push=o,i.prototype.has=r,e.exports=i},6384:function(e,n,t){var a=t(8407),o=t(7465),r=t(3779),i=t(7599),s=t(4758),l=t(4309);function c(e){var n=this.__data__=new a(e);this.size=n.size}c.prototype.clear=o,c.prototype.delete=r,c.prototype.get=i,c.prototype.has=s,c.prototype.set=l,e.exports=c},2705:function(e,n,t){var a=t(5639).Symbol;e.exports=a},1149:function(e,n,t){var a=t(5639).Uint8Array;e.exports=a},577:function(e,n,t){var a=t(852)(t(5639),"WeakMap");e.exports=a},6874:function(e){e.exports=function(e,n,t){switch(t.length){case 0:return e.call(n);case 1:return e.call(n,t[0]);case 2:return e.call(n,t[0],t[1]);case 3:return e.call(n,t[0],t[1],t[2])}return e.apply(n,t)}},4963:function(e){e.exports=function(e,n){for(var t=-1,a=null==e?0:e.length,o=0,r=[];++t<a;){var i=e[t];n(i,t,e)&&(r[o++]=i)}return r}},4636:function(e,n,t){var a=t(2545),o=t(5694),r=t(1469),i=t(4144),s=t(5776),l=t(6719),c=Object.prototype.hasOwnProperty;e.exports=function(e,n){var t=r(e),u=!t&&o(e),d=!t&&!u&&i(e),p=!t&&!u&&!d&&l(e),h=t||u||d||p,f=h?a(e.length,String):[],g=f.length;for(var m in e)!n&&!c.call(e,m)||h&&("length"==m||d&&("offset"==m||"parent"==m)||p&&("buffer"==m||"byteLength"==m||"byteOffset"==m)||s(m,g))||f.push(m);return f}},9932:function(e){e.exports=function(e,n){for(var t=-1,a=null==e?0:e.length,o=Array(a);++t<a;)o[t]=n(e[t],t,e);return o}},2488:function(e){e.exports=function(e,n){for(var t=-1,a=n.length,o=e.length;++t<a;)e[o+t]=n[t];return e}},2908:function(e){e.exports=function(e,n){for(var t=-1,a=null==e?0:e.length;++t<a;)if(n(e[t],t,e))return!0;return!1}},8470:function(e,n,t){var a=t(7813);e.exports=function(e,n){for(var t=e.length;t--;)if(a(e[t][0],n))return t;return-1}},9881:function(e,n,t){var a=t(7816),o=t(9291)(a);e.exports=o},1078:function(e,n,t){var a=t(2488),o=t(7285);e.exports=function e(n,t,r,i,s){var l=-1,c=n.length;for(r||(r=o),s||(s=[]);++l<c;){var u=n[l];t>0&&r(u)?t>1?e(u,t-1,r,i,s):a(s,u):i||(s[s.length]=u)}return s}},8483:function(e,n,t){var a=t(5063)();e.exports=a},7816:function(e,n,t){var a=t(8483),o=t(3674);e.exports=function(e,n){return e&&a(e,n,o)}},7786:function(e,n,t){var a=t(1811),o=t(327);e.exports=function(e,n){for(var t=0,r=(n=a(n,e)).length;null!=e&&t<r;)e=e[o(n[t++])];return t&&t==r?e:void 0}},8866:function(e,n,t){var a=t(2488),o=t(1469);e.exports=function(e,n,t){var r=n(e);return o(e)?r:a(r,t(e))}},4239:function(e,n,t){var a=t(2705),o=t(9607),r=t(2333),i=a?a.toStringTag:void 0;e.exports=function(e){return null==e?void 0===e?"[object Undefined]":"[object Null]":i&&i in Object(e)?o(e):r(e)}},13:function(e){e.exports=function(e,n){return null!=e&&n in Object(e)}},9454:function(e,n,t){var a=t(4239),o=t(7005);e.exports=function(e){return o(e)&&"[object Arguments]"==a(e)}},939:function(e,n,t){var a=t(2492),o=t(7005);e.exports=function e(n,t,r,i,s){return n===t||(null==n||null==t||!o(n)&&!o(t)?n!=n&&t!=t:a(n,t,r,i,e,s))}},2492:function(e,n,t){var a=t(6384),o=t(7114),r=t(8351),i=t(6096),s=t(4160),l=t(1469),c=t(4144),u=t(6719),d="[object Arguments]",p="[object Array]",h="[object Object]",f=Object.prototype.hasOwnProperty;e.exports=function(e,n,t,g,m,b){var v=l(e),y=l(n),w=v?p:s(e),k=y?p:s(n),x=(w=w==d?h:w)==h,C=(k=k==d?h:k)==h,I=w==k;if(I&&c(e)){if(!c(n))return!1;v=!0,x=!1}if(I&&!x)return b||(b=new a),v||u(e)?o(e,n,t,g,m,b):r(e,n,w,t,g,m,b);if(!(1&t)){var L=x&&f.call(e,"__wrapped__"),A=C&&f.call(n,"__wrapped__");if(L||A){var M=L?e.value():e,S=A?n.value():n;return b||(b=new a),m(M,S,t,g,b)}}return!!I&&(b||(b=new a),i(e,n,t,g,m,b))}},2958:function(e,n,t){var a=t(6384),o=t(939);e.exports=function(e,n,t,r){var i=t.length,s=i,l=!r;if(null==e)return!s;for(e=Object(e);i--;){var c=t[i];if(l&&c[2]?c[1]!==e[c[0]]:!(c[0]in e))return!1}for(;++i<s;){var u=(c=t[i])[0],d=e[u],p=c[1];if(l&&c[2]){if(void 0===d&&!(u in e))return!1}else{var h=new a;if(r)var f=r(d,p,u,e,n,h);if(!(void 0===f?o(p,d,3,r,h):f))return!1}}return!0}},8458:function(e,n,t){var a=t(3560),o=t(5346),r=t(3218),i=t(346),s=/^\[object .+?Constructor\]$/,l=Function.prototype,c=Object.prototype,u=l.toString,d=c.hasOwnProperty,p=RegExp("^"+u.call(d).replace(/[\\^$.*+?()[\]{}|]/g,"\\$&").replace(/hasOwnProperty|(function).*?(?=\\\()| for .+?(?=\\\])/g,"$1.*?")+"$");e.exports=function(e){return!(!r(e)||o(e))&&(a(e)?p:s).test(i(e))}},8749:function(e,n,t){var a=t(4239),o=t(1780),r=t(7005),i={};i["[object Float32Array]"]=i["[object Float64Array]"]=i["[object Int8Array]"]=i["[object Int16Array]"]=i["[object Int32Array]"]=i["[object Uint8Array]"]=i["[object Uint8ClampedArray]"]=i["[object Uint16Array]"]=i["[object Uint32Array]"]=!0,i["[object Arguments]"]=i["[object Array]"]=i["[object ArrayBuffer]"]=i["[object Boolean]"]=i["[object DataView]"]=i["[object Date]"]=i["[object Error]"]=i["[object Function]"]=i["[object Map]"]=i["[object Number]"]=i["[object Object]"]=i["[object RegExp]"]=i["[object Set]"]=i["[object String]"]=i["[object WeakMap]"]=!1,e.exports=function(e){return r(e)&&o(e.length)&&!!i[a(e)]}},7206:function(e,n,t){var a=t(1573),o=t(6432),r=t(6557),i=t(1469),s=t(9601);e.exports=function(e){return"function"==typeof e?e:null==e?r:"object"==typeof e?i(e)?o(e[0],e[1]):a(e):s(e)}},280:function(e,n,t){var a=t(5726),o=t(6916),r=Object.prototype.hasOwnProperty;e.exports=function(e){if(!a(e))return o(e);var n=[];for(var t in Object(e))r.call(e,t)&&"constructor"!=t&&n.push(t);return n}},9199:function(e,n,t){var a=t(9881),o=t(8612);e.exports=function(e,n){var t=-1,r=o(e)?Array(e.length):[];return a(e,(function(e,a,o){r[++t]=n(e,a,o)})),r}},1573:function(e,n,t){var a=t(2958),o=t(1499),r=t(2634);e.exports=function(e){var n=o(e);return 1==n.length&&n[0][2]?r(n[0][0],n[0][1]):function(t){return t===e||a(t,e,n)}}},6432:function(e,n,t){var a=t(939),o=t(7361),r=t(9095),i=t(5403),s=t(9162),l=t(2634),c=t(327);e.exports=function(e,n){return i(e)&&s(n)?l(c(e),n):function(t){var i=o(t,e);return void 0===i&&i===n?r(t,e):a(n,i,3)}}},2689:function(e,n,t){var a=t(9932),o=t(7786),r=t(7206),i=t(9199),s=t(1131),l=t(1717),c=t(5022),u=t(6557),d=t(1469);e.exports=function(e,n,t){n=n.length?a(n,(function(e){return d(e)?function(n){return o(n,1===e.length?e[0]:e)}:e})):[u];var p=-1;n=a(n,l(r));var h=i(e,(function(e,t,o){return{criteria:a(n,(function(n){return n(e)})),index:++p,value:e}}));return s(h,(function(e,n){return c(e,n,t)}))}},371:function(e){e.exports=function(e){return function(n){return null==n?void 0:n[e]}}},9152:function(e,n,t){var a=t(7786);e.exports=function(e){return function(n){return a(n,e)}}},5976:function(e,n,t){var a=t(6557),o=t(5357),r=t(61);e.exports=function(e,n){return r(o(e,n,a),e+"")}},6560:function(e,n,t){var a=t(5703),o=t(8777),r=t(6557),i=o?function(e,n){return o(e,"toString",{configurable:!0,enumerable:!1,value:a(n),writable:!0})}:r;e.exports=i},1131:function(e){e.exports=function(e,n){var t=e.length;for(e.sort(n);t--;)e[t]=e[t].value;return e}},2545:function(e){e.exports=function(e,n){for(var t=-1,a=Array(e);++t<e;)a[t]=n(t);return a}},531:function(e,n,t){var a=t(2705),o=t(9932),r=t(1469),i=t(3448),s=a?a.prototype:void 0,l=s?s.toString:void 0;e.exports=function e(n){if("string"==typeof n)return n;if(r(n))return o(n,e)+"";if(i(n))return l?l.call(n):"";var t=n+"";return"0"==t&&1/n==-Infinity?"-0":t}},1717:function(e){e.exports=function(e){return function(n){return e(n)}}},4757:function(e){e.exports=function(e,n){return e.has(n)}},1811:function(e,n,t){var a=t(1469),o=t(5403),r=t(5514),i=t(9833);e.exports=function(e,n){return a(e)?e:o(e,n)?[e]:r(i(e))}},6393:function(e,n,t){var a=t(3448);e.exports=function(e,n){if(e!==n){var t=void 0!==e,o=null===e,r=e==e,i=a(e),s=void 0!==n,l=null===n,c=n==n,u=a(n);if(!l&&!u&&!i&&e>n||i&&s&&c&&!l&&!u||o&&s&&c||!t&&c||!r)return 1;if(!o&&!i&&!u&&e<n||u&&t&&r&&!o&&!i||l&&t&&r||!s&&r||!c)return-1}return 0}},5022:function(e,n,t){var a=t(6393);e.exports=function(e,n,t){for(var o=-1,r=e.criteria,i=n.criteria,s=r.length,l=t.length;++o<s;){var c=a(r[o],i[o]);if(c)return o>=l?c:c*("desc"==t[o]?-1:1)}return e.index-n.index}},4429:function(e,n,t){var a=t(5639)["__core-js_shared__"];e.exports=a},9291:function(e,n,t){var a=t(8612);e.exports=function(e,n){return function(t,o){if(null==t)return t;if(!a(t))return e(t,o);for(var r=t.length,i=n?r:-1,s=Object(t);(n?i--:++i<r)&&!1!==o(s[i],i,s););return t}}},5063:function(e){e.exports=function(e){return function(n,t,a){for(var o=-1,r=Object(n),i=a(n),s=i.length;s--;){var l=i[e?s:++o];if(!1===t(r[l],l,r))break}return n}}},8777:function(e,n,t){var a=t(852),o=function(){try{var e=a(Object,"defineProperty");return e({},"",{}),e}catch(n){}}();e.exports=o},7114:function(e,n,t){var a=t(8668),o=t(2908),r=t(4757);e.exports=function(e,n,t,i,s,l){var c=1&t,u=e.length,d=n.length;if(u!=d&&!(c&&d>u))return!1;var p=l.get(e),h=l.get(n);if(p&&h)return p==n&&h==e;var f=-1,g=!0,m=2&t?new a:void 0;for(l.set(e,n),l.set(n,e);++f<u;){var b=e[f],v=n[f];if(i)var y=c?i(v,b,f,n,e,l):i(b,v,f,e,n,l);if(void 0!==y){if(y)continue;g=!1;break}if(m){if(!o(n,(function(e,n){if(!r(m,n)&&(b===e||s(b,e,t,i,l)))return m.push(n)}))){g=!1;break}}else if(b!==v&&!s(b,v,t,i,l)){g=!1;break}}return l.delete(e),l.delete(n),g}},8351:function(e,n,t){var a=t(2705),o=t(1149),r=t(7813),i=t(7114),s=t(8776),l=t(1814),c=a?a.prototype:void 0,u=c?c.valueOf:void 0;e.exports=function(e,n,t,a,c,d,p){switch(t){case"[object DataView]":if(e.byteLength!=n.byteLength||e.byteOffset!=n.byteOffset)return!1;e=e.buffer,n=n.buffer;case"[object ArrayBuffer]":return!(e.byteLength!=n.byteLength||!d(new o(e),new o(n)));case"[object Boolean]":case"[object Date]":case"[object Number]":return r(+e,+n);case"[object Error]":return e.name==n.name&&e.message==n.message;case"[object RegExp]":case"[object String]":return e==n+"";case"[object Map]":var h=s;case"[object Set]":var f=1&a;if(h||(h=l),e.size!=n.size&&!f)return!1;var g=p.get(e);if(g)return g==n;a|=2,p.set(e,n);var m=i(h(e),h(n),a,c,d,p);return p.delete(e),m;case"[object Symbol]":if(u)return u.call(e)==u.call(n)}return!1}},6096:function(e,n,t){var a=t(8234),o=Object.prototype.hasOwnProperty;e.exports=function(e,n,t,r,i,s){var l=1&t,c=a(e),u=c.length;if(u!=a(n).length&&!l)return!1;for(var d=u;d--;){var p=c[d];if(!(l?p in n:o.call(n,p)))return!1}var h=s.get(e),f=s.get(n);if(h&&f)return h==n&&f==e;var g=!0;s.set(e,n),s.set(n,e);for(var m=l;++d<u;){var b=e[p=c[d]],v=n[p];if(r)var y=l?r(v,b,p,n,e,s):r(b,v,p,e,n,s);if(!(void 0===y?b===v||i(b,v,t,r,s):y)){g=!1;break}m||(m="constructor"==p)}if(g&&!m){var w=e.constructor,k=n.constructor;w==k||!("constructor"in e)||!("constructor"in n)||"function"==typeof w&&w instanceof w&&"function"==typeof k&&k instanceof k||(g=!1)}return s.delete(e),s.delete(n),g}},1957:function(e,n,t){var a="object"==typeof t.g&&t.g&&t.g.Object===Object&&t.g;e.exports=a},8234:function(e,n,t){var a=t(8866),o=t(9551),r=t(3674);e.exports=function(e){return a(e,r,o)}},5050:function(e,n,t){var a=t(7019);e.exports=function(e,n){var t=e.__data__;return a(n)?t["string"==typeof n?"string":"hash"]:t.map}},1499:function(e,n,t){var a=t(9162),o=t(3674);e.exports=function(e){for(var n=o(e),t=n.length;t--;){var r=n[t],i=e[r];n[t]=[r,i,a(i)]}return n}},852:function(e,n,t){var a=t(8458),o=t(7801);e.exports=function(e,n){var t=o(e,n);return a(t)?t:void 0}},9607:function(e,n,t){var a=t(2705),o=Object.prototype,r=o.hasOwnProperty,i=o.toString,s=a?a.toStringTag:void 0;e.exports=function(e){var n=r.call(e,s),t=e[s];try{e[s]=void 0;var a=!0}catch(l){}var o=i.call(e);return a&&(n?e[s]=t:delete e[s]),o}},9551:function(e,n,t){var a=t(4963),o=t(479),r=Object.prototype.propertyIsEnumerable,i=Object.getOwnPropertySymbols,s=i?function(e){return null==e?[]:(e=Object(e),a(i(e),(function(n){return r.call(e,n)})))}:o;e.exports=s},4160:function(e,n,t){var a=t(8552),o=t(7071),r=t(3818),i=t(8525),s=t(577),l=t(4239),c=t(346),u="[object Map]",d="[object Promise]",p="[object Set]",h="[object WeakMap]",f="[object DataView]",g=c(a),m=c(o),b=c(r),v=c(i),y=c(s),w=l;(a&&w(new a(new ArrayBuffer(1)))!=f||o&&w(new o)!=u||r&&w(r.resolve())!=d||i&&w(new i)!=p||s&&w(new s)!=h)&&(w=function(e){var n=l(e),t="[object Object]"==n?e.constructor:void 0,a=t?c(t):"";if(a)switch(a){case g:return f;case m:return u;case b:return d;case v:return p;case y:return h}return n}),e.exports=w},7801:function(e){e.exports=function(e,n){return null==e?void 0:e[n]}},222:function(e,n,t){var a=t(1811),o=t(5694),r=t(1469),i=t(5776),s=t(1780),l=t(327);e.exports=function(e,n,t){for(var c=-1,u=(n=a(n,e)).length,d=!1;++c<u;){var p=l(n[c]);if(!(d=null!=e&&t(e,p)))break;e=e[p]}return d||++c!=u?d:!!(u=null==e?0:e.length)&&s(u)&&i(p,u)&&(r(e)||o(e))}},1789:function(e,n,t){var a=t(4536);e.exports=function(){this.__data__=a?a(null):{},this.size=0}},401:function(e){e.exports=function(e){var n=this.has(e)&&delete this.__data__[e];return this.size-=n?1:0,n}},7667:function(e,n,t){var a=t(4536),o=Object.prototype.hasOwnProperty;e.exports=function(e){var n=this.__data__;if(a){var t=n[e];return"__lodash_hash_undefined__"===t?void 0:t}return o.call(n,e)?n[e]:void 0}},1327:function(e,n,t){var a=t(4536),o=Object.prototype.hasOwnProperty;e.exports=function(e){var n=this.__data__;return a?void 0!==n[e]:o.call(n,e)}},1866:function(e,n,t){var a=t(4536);e.exports=function(e,n){var t=this.__data__;return this.size+=this.has(e)?0:1,t[e]=a&&void 0===n?"__lodash_hash_undefined__":n,this}},7285:function(e,n,t){var a=t(2705),o=t(5694),r=t(1469),i=a?a.isConcatSpreadable:void 0;e.exports=function(e){return r(e)||o(e)||!!(i&&e&&e[i])}},5776:function(e){var n=/^(?:0|[1-9]\d*)$/;e.exports=function(e,t){var a=typeof e;return!!(t=null==t?9007199254740991:t)&&("number"==a||"symbol"!=a&&n.test(e))&&e>-1&&e%1==0&&e<t}},6612:function(e,n,t){var a=t(7813),o=t(8612),r=t(5776),i=t(3218);e.exports=function(e,n,t){if(!i(t))return!1;var s=typeof n;return!!("number"==s?o(t)&&r(n,t.length):"string"==s&&n in t)&&a(t[n],e)}},5403:function(e,n,t){var a=t(1469),o=t(3448),r=/\.|\[(?:[^[\]]*|(["'])(?:(?!\1)[^\\]|\\.)*?\1)\]/,i=/^\w*$/;e.exports=function(e,n){if(a(e))return!1;var t=typeof e;return!("number"!=t&&"symbol"!=t&&"boolean"!=t&&null!=e&&!o(e))||(i.test(e)||!r.test(e)||null!=n&&e in Object(n))}},7019:function(e){e.exports=function(e){var n=typeof e;return"string"==n||"number"==n||"symbol"==n||"boolean"==n?"__proto__"!==e:null===e}},5346:function(e,n,t){var a,o=t(4429),r=(a=/[^.]+$/.exec(o&&o.keys&&o.keys.IE_PROTO||""))?"Symbol(src)_1."+a:"";e.exports=function(e){return!!r&&r in e}},5726:function(e){var n=Object.prototype;e.exports=function(e){var t=e&&e.constructor;return e===("function"==typeof t&&t.prototype||n)}},9162:function(e,n,t){var a=t(3218);e.exports=function(e){return e==e&&!a(e)}},7040:function(e){e.exports=function(){this.__data__=[],this.size=0}},4125:function(e,n,t){var a=t(8470),o=Array.prototype.splice;e.exports=function(e){var n=this.__data__,t=a(n,e);return!(t<0)&&(t==n.length-1?n.pop():o.call(n,t,1),--this.size,!0)}},2117:function(e,n,t){var a=t(8470);e.exports=function(e){var n=this.__data__,t=a(n,e);return t<0?void 0:n[t][1]}},7518:function(e,n,t){var a=t(8470);e.exports=function(e){return a(this.__data__,e)>-1}},4705:function(e,n,t){var a=t(8470);e.exports=function(e,n){var t=this.__data__,o=a(t,e);return o<0?(++this.size,t.push([e,n])):t[o][1]=n,this}},4785:function(e,n,t){var a=t(1989),o=t(8407),r=t(7071);e.exports=function(){this.size=0,this.__data__={hash:new a,map:new(r||o),string:new a}}},1285:function(e,n,t){var a=t(5050);e.exports=function(e){var n=a(this,e).delete(e);return this.size-=n?1:0,n}},6e3:function(e,n,t){var a=t(5050);e.exports=function(e){return a(this,e).get(e)}},9916:function(e,n,t){var a=t(5050);e.exports=function(e){return a(this,e).has(e)}},5265:function(e,n,t){var a=t(5050);e.exports=function(e,n){var t=a(this,e),o=t.size;return t.set(e,n),this.size+=t.size==o?0:1,this}},8776:function(e){e.exports=function(e){var n=-1,t=Array(e.size);return e.forEach((function(e,a){t[++n]=[a,e]})),t}},2634:function(e){e.exports=function(e,n){return function(t){return null!=t&&(t[e]===n&&(void 0!==n||e in Object(t)))}}},4523:function(e,n,t){var a=t(8306);e.exports=function(e){var n=a(e,(function(e){return 500===t.size&&t.clear(),e})),t=n.cache;return n}},4536:function(e,n,t){var a=t(852)(Object,"create");e.exports=a},6916:function(e,n,t){var a=t(5569)(Object.keys,Object);e.exports=a},1167:function(e,n,t){e=t.nmd(e);var a=t(1957),o=n&&!n.nodeType&&n,r=o&&e&&!e.nodeType&&e,i=r&&r.exports===o&&a.process,s=function(){try{var e=r&&r.require&&r.require("util").types;return e||i&&i.binding&&i.binding("util")}catch(n){}}();e.exports=s},2333:function(e){var n=Object.prototype.toString;e.exports=function(e){return n.call(e)}},5569:function(e){e.exports=function(e,n){return function(t){return e(n(t))}}},5357:function(e,n,t){var a=t(6874),o=Math.max;e.exports=function(e,n,t){return n=o(void 0===n?e.length-1:n,0),function(){for(var r=arguments,i=-1,s=o(r.length-n,0),l=Array(s);++i<s;)l[i]=r[n+i];i=-1;for(var c=Array(n+1);++i<n;)c[i]=r[i];return c[n]=t(l),a(e,this,c)}}},5639:function(e,n,t){var a=t(1957),o="object"==typeof self&&self&&self.Object===Object&&self,r=a||o||Function("return this")();e.exports=r},619:function(e){e.exports=function(e){return this.__data__.set(e,"__lodash_hash_undefined__"),this}},2385:function(e){e.exports=function(e){return this.__data__.has(e)}},1814:function(e){e.exports=function(e){var n=-1,t=Array(e.size);return e.forEach((function(e){t[++n]=e})),t}},61:function(e,n,t){var a=t(6560),o=t(1275)(a);e.exports=o},1275:function(e){var n=Date.now;e.exports=function(e){var t=0,a=0;return function(){var o=n(),r=16-(o-a);if(a=o,r>0){if(++t>=800)return arguments[0]}else t=0;return e.apply(void 0,arguments)}}},7465:function(e,n,t){var a=t(8407);e.exports=function(){this.__data__=new a,this.size=0}},3779:function(e){e.exports=function(e){var n=this.__data__,t=n.delete(e);return this.size=n.size,t}},7599:function(e){e.exports=function(e){return this.__data__.get(e)}},4758:function(e){e.exports=function(e){return this.__data__.has(e)}},4309:function(e,n,t){var a=t(8407),o=t(7071),r=t(3369);e.exports=function(e,n){var t=this.__data__;if(t instanceof a){var i=t.__data__;if(!o||i.length<199)return i.push([e,n]),this.size=++t.size,this;t=this.__data__=new r(i)}return t.set(e,n),this.size=t.size,this}},5514:function(e,n,t){var a=t(4523),o=/[^.[\]]+|\[(?:(-?\d+(?:\.\d+)?)|(["'])((?:(?!\2)[^\\]|\\.)*?)\2)\]|(?=(?:\.|\[\])(?:\.|\[\]|$))/g,r=/\\(\\)?/g,i=a((function(e){var n=[];return 46===e.charCodeAt(0)&&n.push(""),e.replace(o,(function(e,t,a,o){n.push(a?o.replace(r,"$1"):t||e)})),n}));e.exports=i},327:function(e,n,t){var a=t(3448);e.exports=function(e){if("string"==typeof e||a(e))return e;var n=e+"";return"0"==n&&1/e==-Infinity?"-0":n}},346:function(e){var n=Function.prototype.toString;e.exports=function(e){if(null!=e){try{return n.call(e)}catch(t){}try{return e+""}catch(t){}}return""}},5703:function(e){e.exports=function(e){return function(){return e}}},7813:function(e){e.exports=function(e,n){return e===n||e!=e&&n!=n}},7361:function(e,n,t){var a=t(7786);e.exports=function(e,n,t){var o=null==e?void 0:a(e,n);return void 0===o?t:o}},9095:function(e,n,t){var a=t(13),o=t(222);e.exports=function(e,n){return null!=e&&o(e,n,a)}},6557:function(e){e.exports=function(e){return e}},5694:function(e,n,t){var a=t(9454),o=t(7005),r=Object.prototype,i=r.hasOwnProperty,s=r.propertyIsEnumerable,l=a(function(){return arguments}())?a:function(e){return o(e)&&i.call(e,"callee")&&!s.call(e,"callee")};e.exports=l},1469:function(e){var n=Array.isArray;e.exports=n},8612:function(e,n,t){var a=t(3560),o=t(1780);e.exports=function(e){return null!=e&&o(e.length)&&!a(e)}},4144:function(e,n,t){e=t.nmd(e);var a=t(5639),o=t(5062),r=n&&!n.nodeType&&n,i=r&&e&&!e.nodeType&&e,s=i&&i.exports===r?a.Buffer:void 0,l=(s?s.isBuffer:void 0)||o;e.exports=l},3560:function(e,n,t){var a=t(4239),o=t(3218);e.exports=function(e){if(!o(e))return!1;var n=a(e);return"[object Function]"==n||"[object GeneratorFunction]"==n||"[object AsyncFunction]"==n||"[object Proxy]"==n}},1780:function(e){e.exports=function(e){return"number"==typeof e&&e>-1&&e%1==0&&e<=9007199254740991}},3218:function(e){e.exports=function(e){var n=typeof e;return null!=e&&("object"==n||"function"==n)}},7005:function(e){e.exports=function(e){return null!=e&&"object"==typeof e}},3448:function(e,n,t){var a=t(4239),o=t(7005);e.exports=function(e){return"symbol"==typeof e||o(e)&&"[object Symbol]"==a(e)}},6719:function(e,n,t){var a=t(8749),o=t(1717),r=t(1167),i=r&&r.isTypedArray,s=i?o(i):a;e.exports=s},3674:function(e,n,t){var a=t(4636),o=t(280),r=t(8612);e.exports=function(e){return r(e)?a(e):o(e)}},8306:function(e,n,t){var a=t(3369);function o(e,n){if("function"!=typeof e||null!=n&&"function"!=typeof n)throw new TypeError("Expected a function");var t=function(){var a=arguments,o=n?n.apply(this,a):a[0],r=t.cache;if(r.has(o))return r.get(o);var i=e.apply(this,a);return t.cache=r.set(o,i)||r,i};return t.cache=new(o.Cache||a),t}o.Cache=a,e.exports=o},9601:function(e,n,t){var a=t(371),o=t(9152),r=t(5403),i=t(327);e.exports=function(e){return r(e)?a(i(e)):o(e)}},1351:function(e){var n=Array.prototype.reverse;e.exports=function(e){return null==e?e:n.call(e)}},9734:function(e,n,t){var a=t(1078),o=t(2689),r=t(5976),i=t(6612),s=r((function(e,n){if(null==e)return[];var t=n.length;return t>1&&i(e,n[0],n[1])?n=[]:t>2&&i(n[0],n[1],n[2])&&(n=[n[0]]),o(e,a(n,1),[])}));e.exports=s},479:function(e){e.exports=function(){return[]}},5062:function(e){e.exports=function(){return!1}},9833:function(e,n,t){var a=t(531);e.exports=function(e){return null==e?"":a(e)}}}]);
//# sourceMappingURL=7a1b4902e6bb6fb420d1a730bb79c9fa23af83b5-a14e4469c5e71b7e4a8a.js.map
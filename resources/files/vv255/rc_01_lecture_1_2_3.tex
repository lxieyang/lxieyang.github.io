\documentclass{beamer}
\usetheme{default}
\usepackage{float}
\usepackage{animate}
%\usepackage[T1]{fontenc}

%IndentfFirst 
%\usepackage{indentfirst}
%\setlength{\parindent}{2em}
%\setlength{\parskip}{2em}


%Geometry
\usepackage{geometry}
\geometry{left = 0.25in,right = 0.25in}



%Adjust frametitle position (height,lateral position, etc)
\defbeamertemplate*{frametitle}{smoothbars theme}
{%
	%\nointerlineskip%
	\begin{beamercolorbox}[wd=\paperwidth,leftskip=.5cm,rightskip=.3cm plus1fil,vmode]{frametitle}
		\vskip +4.5ex
		\usebeamerfont*{frametitle}\insertframetitle%
		\vskip -0.9ex
	\end{beamercolorbox}%
}


\begin{document}


%headline

\setbeamertemplate{headline}{
\parbox{\linewidth}{\vspace*{8pt}\centering{ 
		    {\color{blue!40!black}\insertsection}
		}
	}
}


%footline
\setbeamertemplate{footline}[text line]{%
	\color{blue!40!black}\parbox{\linewidth}{\vspace*{-8pt}Michael Liu ~ (\insertshortinstitute)~~~~~~~~~~~~~~~~~~~~~~~~\insertshorttitle\hfill\insertshortdate~~~~~~\insertframenumber{}~/~\inserttotalframenumber}}

%Remove navigation symbols
\setbeamertemplate{navigation symbols}{}

\setbeamertemplate{frametitle continuation}[from second] 

\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}

%\let\oldframe\frame\renewcommand\frame[1][allowframebreaks]{\oldframe[#1]}

%Title page	
\title[Vv255 Applied Calculus III]{Vv255 Applied Calculus III\\{\small Recitation I}}   
\author[Michael Liu]{LIU Xieyang\\{\tiny Teaching Assistant}} 
\institute[UM-SJTU JI]{University of Michigan - Shanghai Jiaotong University \\Joint Institute}
\date[Summer 2015]{Summer Term 2015} 
\begin{frame}
	\titlepage
\end{frame}

%Table of Contents (All)
\begin{frame}
	\frametitle{Table of Contents}
	\tableofcontents
\end{frame}

%Table of contents (before, highlight each section)
%\AtBeginSection[]{
%	\begin{frame}
		%\frametitle{Table of Contents}
%		\frametitle{Contents}
%		\tableofcontents[currentsection]
%	\end{frame}




%section
\section{Lecture 1: Vectors, Matrices and Linear equations} 



\begin{frame}[allowframebreaks]{Multivariable calculus}
	Multivariable calculus (A.K.A. multivariate calculus) is the extension of calculus
	in one variable to calculus in more than one variable.
\begin{itemize}
	\item Pay attention to the figures on Page 2, Lecture 1
\end{itemize}
\begin{enumerate}
	\item Different \alert{dimensions} will lead to different meanings/interpretations of the \alert{same} equation.
	\begin{itemize}
		\item For example $x^2 + y^2 = 1$ in $\mathbb{R}^2$ and $\mathbb{R}^3$.
	\end{itemize}
	
	\item In 2D analytical geometry, the graph of an equation involving $x$ and $y$ is a \alert{curve} in $\mathbb{R}^2$. \\In 3D analytical geometry, an equation in $x$, $y$, and $z$ represents a \alert{surface} in $\mathbb{R}^3$.
	
	\item Distance formula in $\mathbb{R}^3$: $|P_1P_2| = \sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2 + (z_2-z_1)^2}$
\end{enumerate}


\end{frame}



\begin{frame}[allowframebreaks]{Vector}
\begin{enumerate}
	\item Definition: \\
	The term vector is used to indicate a quantity that has both
	magnitude and direction .
	\begin{itemize}
		\item Equal/Equivalence: \alert{same magnitude \& direction}.
		\item $\mathbf{0}$ is the only vector that has \alert{no specific direction}.
	\end{itemize}
	
	\item Manipulation:
	\begin{itemize}
		\item Addition
		\item Scalar Multiplication (eg. \alert{scalar multiple $\Rightarrow$ parallel})
		\item length or magnitude
	\end{itemize}
	
	\item \alert{8} defining properties (axioms) of addition and scalar multiplication: see Page 7, Lecture 1.
	
	\item {\color{green} Vector Space}:
	A set $V$ satisfies the following two additional axioms as well as the eight axioms above is known as a {\color{green} vector space}.
	\begin{itemize}
		\item If $\mathbf{u}$ and $\mathbf{v}$ are in $V$, then the sum of $\mathbf{u}$ and $\mathbf{v}$ is in $V$.
		\item If $\mathbf{u}$ is in $V$, then the scalar multiple of $\mathbf{u}$ by $\alpha$ is in $V$.
	\end{itemize}
	
\end{enumerate}

	
\end{frame}



\begin{frame}[allowframebreaks]{Matrices}
	\begin{enumerate}
		\item Definition:
		\begin{itemize}
			\item \alert{Row-major propertity}: By an $m \times n$ matrix (read $m$ by $n$ matrix) we mean a matrix with $m$ rows and $n$ columns.
		\end{itemize}
		
		\item Manipulation:
		\begin{itemize}
			\item Addition of Matrices: \alert{same size!}
			\item Scalar Multiplication
		\end{itemize}
		
		\item 8 properties of Matrices addition and scalar multiplication
		\begin{itemize}
			\item Here $\mathbf{0}$ denote the zero matrix of the \alert{right size}.
		\end{itemize}
		
		\item Multiplication of a Matrix by a Matrix:  
		\begin{itemize}
			\item The no. of columns of the first matrix = the no. of rows of the second matrix (\alert{size matches!})
			\item Properties: \alert{associativity}, \alert{left \& right distributivity}
			\item Identity matrix
		\end{itemize}
		
		\item Three ways of looking at a linear system:
		\begin{itemize}
			\item Row (Intersection)~ Column (Combination)~ Matrix (Inverse image)
		\end{itemize}
		
	\end{enumerate}
	
	
\end{frame}








\section{Lecture 2: Dot products, Projections and Bases}

\begin{frame}[allowframebreaks]{Dot  (inner) product}
	\begin{enumerate}
		\item Dot product is a \alert{scalar} quantity!
		
		
		\item Properties of the dot product
		\begin{itemize}
			\item Magnitude: $|\mathbf{v}| = \sqrt{\mathbf{v}\cdot\mathbf{v}}$
			\item Normalizing: $\hat{\mathbf{v}} = \dfrac{\mathbf{v}}{|\mathbf{v}|} = \dfrac{\mathbf{v}}{\sqrt{\mathbf{v}\cdot\mathbf{v}}}$
			
		\end{itemize}
		
		\item Geometric definition: $\mathbf{u}\cdot\mathbf{v} = |\mathbf{u}|\cdot|\mathbf{v}|\cos\theta$
		\begin{itemize}
			\item Cauchy-Schwarz inequality
			\item Triangle Inequality
			\item Parallelogram Law
		\end{itemize}
		\item Orthogonal \& Orthonormal
		\begin{itemize}
			\item \alert{The dot product measures the extent to which two vectors point in
				the same general direction.}
		\end{itemize}
		
		\item Scalar and Vector projection
		\begin{itemize}
			\item Parallel and perpendicular components
		\end{itemize}
	\end{enumerate}
	
	
\end{frame}




\begin{frame}[allowframebreaks]{Bases}
	\begin{enumerate}
		\item Basis in $\mathbb{R}^3$ must be \alert{non-coplanar}.
		\item *Linear independence: The vectors $\mathbf{a}_1$, $\mathbf{a}_2$, $\cdots$, $\mathbf{a}_k$ for some integer $k$ are \alert{linearly independent} if the
		only way to satisfy the following equation
		$$\alpha_1\mathbf{a}_1 + \alpha_2\mathbf{a}_2 + \cdots + \alpha_k\mathbf{a}_k = \mathbf{0}$$
		is for all $\alpha$'s to be zero.
		\item *Dimension: The dimension of a space is the largest number of linearly independent vectors, $n$, in that space. A basis for that space consists of $n$ linearly independent vectors. A vector $\mathbf{v}$ in that space has $n$ components (some of them possibly zero) with
		respect to any basis in that space.
		
		\item Orthonormal basis in $\mathbb{R}^3$
		
		\item {\color{green} Basis Transformation}: see worksheet
		
	\end{enumerate}
	
	
\end{frame}






\section{Lecture 3: Cross products, Determinants and Eigenvalues}




\begin{frame}[allowframebreaks]{Cross Product}
	\begin{enumerate}
		\item Definition of cross product: 
		\begin{itemize}
			\item Magnitude: the area of
			the parallelogram ($|\mathbf{u}\times\mathbf{v}| = |\mathbf{u}||\mathbf{v}|\sin\theta$)
			\item Direction: right-hand rule
			\item The vector $\mathbf{u}\times\mathbf{v}$ is perpendicular to both nonzero vectors $\mathbf{u}$ and $\mathbf{v}$, actually it's perpendicular to all the vectors in the plane formed by $\mathbf{u}$ and $\mathbf{v}$.
			\end{itemize}
			\item Determination of parallel \& perpendicular:
			\begin{itemize}
				\item Parallel: $\mathbf{u}\times\mathbf{v} = \mathbf{0}$
				\item Perpendicular: $\mathbf{u}\cdot\mathbf{v} = 0$
			\end{itemize}
			\item Properties: see page 7, lecture 3.
			\begin{itemize}
				\item \alert{non-commutative}
				\item \alert{non-associative}
				\item left \& right distributive
			\end{itemize}
		
		\item Scalar triple product $\mathbf{w}\cdot(\mathbf{u}\times\mathbf{v})$:\alert{ invariant under circular shift}!
		
		\item {\color{green} Vector triple product}: $\mathbf{a}\times(\mathbf{b}\times\mathbf{c})$
		\begin{itemize}
			\item ``Back-cab rule'': $\mathbf{a}\times(\mathbf{b}\times\mathbf{c}) = \mathbf{b}(\mathbf{a}\cdot\mathbf{c}) - \mathbf{c}(\mathbf{a}\cdot\mathbf{b})$
		\end{itemize}
	\end{enumerate}
	
	
\end{frame}



\begin{frame}[allowframebreaks]{Determinant}
	\begin{enumerate}
		\item Only \alert{square matrix} has determinant.
		
		\item A third-order can be defined in terms of second-order determinants.
		\begin{figure}[H]
			\centering
			\includegraphics[scale = 0.35]{determinant.png}
		\end{figure}
		
		\item Physical meaning:
		\begin{itemize}
			\item \alert{2D}: Theorem \& its proof on page 10, lecture 3:
			\begin{figure}[H]
				\centering
				\includegraphics[scale = 0.35]{parthm.png}
			\end{figure}
			
			\item \alert{3D}:  Theorem \& its proof on page 11, lecture 3:
			\begin{figure}[H]
				\centering
				\includegraphics[scale = 0.35]{par2thm.png}
			\end{figure}
		\end{itemize}
		
		\item *What does it mean to have a determinant equal to zero?
		\begin{itemize}
			\item {\color{blue} zero area} $\Rightarrow$ {\color{red} collinear}
			\item {\color{blue} zero volume} $\Rightarrow$ {\color{red} collinear / coplanar}
			\item $\det(A) = \det(A^T)$ (proof?)
			\item *If at least one of the row matrix in the original matrix can be represented by a \alert{linear combination} of the other row matrices, the determinant of the original matrix must be \alert{0}; in other words, the rows of the original matrix are \alert{not }linearly independent.
		\end{itemize}
		
	\end{enumerate}
	
\end{frame}



\begin{frame}[allowframebreaks]{Eigenvalue \& Eignevector}
	\begin{enumerate}
		\item Definition:
		\begin{figure}[H]
			\centering
			\includegraphics[scale = 0.4]{eigdef.png}
		\end{figure}
		
		\item A common question: page 16, lecture 3:
		\begin{figure}[H]
			\centering
			\includegraphics[scale = 0.43]{16.png}
		\end{figure}
		Think about the definition of \alert{linear independence}, and assume that if $\mathbf{r}_1$ to $\mathbf{r}_n$ are linear independent, what would happen (contrary to the definition of linear independence) ?
		
		\item Calculating eigenvalues and eigenvectors:
		\begin{itemize}
			\item Practice!
		\end{itemize}
	\end{enumerate}

\end{frame}




\end{document}